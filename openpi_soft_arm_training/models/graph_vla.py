#!/usr/bin/env python3
"""
Graph-based VLAæ¨¡å‹
å®ç°ä½ æå‡ºçš„æ¶æ„: å›¾ç¼–ç  + åŒè·¯èåˆ + èŠ‚ç‚¹å¼åŠ¨ä½œå¤´ + Flow Matching
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Any, Optional
import math

# æ·»åŠ OpenPiè·¯å¾„
sys.path.append('/home/cx/AET_FOR_RL/vla/å‚è€ƒæ¨¡å‹/openpi')
from openpi.shared.config import checkpoint_to_config
from openpi.models import pi0_5

class SoftArmGraphNN(nn.Module):
    """è½¯ä½“è‡‚å›¾ç¥ç»ç½‘ç»œ

    è¾“å…¥: URDFå›¾ç»“æ„ (N, 19)
    è¾“å‡º: å›¾token (N, 32)
    """

    def __init__(self,
                 input_dim: int = 19,
                 hidden_dim: int = 64,
                 output_dim: int = 32,
                 num_layers: int = 3):
        super().__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers

        # èŠ‚ç‚¹ç‰¹å¾ç¼–ç 
        self.node_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )

        # å›¾å·ç§¯å±‚
        self.graph_layers = nn.ModuleList([
            GraphConvLayer(hidden_dim, hidden_dim)
            for _ in range(num_layers)
        ])

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, output_dim)

        # å±‚å½’ä¸€åŒ–
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(hidden_dim)
            for _ in range(num_layers)
        ])

    def forward(self,
                node_features: torch.Tensor,  # (B, N, 19)
                edge_indices: torch.Tensor,   # (B, 2, E)
                batch_size: int) -> torch.Tensor:  # (B, N, 32)

        B, N, _ = node_features.shape

        # èŠ‚ç‚¹ç¼–ç 
        x = self.node_encoder(node_features)  # (B, N, hidden_dim)

        # å›¾å·ç§¯ä¼ æ’­
        for i, (conv_layer, norm_layer) in enumerate(zip(self.graph_layers, self.layer_norms)):
            residual = x
            x = conv_layer(x, edge_indices)
            x = norm_layer(x + residual)  # æ®‹å·®è¿æ¥
            x = F.relu(x)

        # è¾“å‡ºæŠ•å½±
        graph_tokens = self.output_proj(x)  # (B, N, output_dim)

        return graph_tokens

class GraphConvLayer(nn.Module):
    """å›¾å·ç§¯å±‚"""

    def __init__(self, in_dim: int, out_dim: int):
        super().__init__()
        self.linear = nn.Linear(in_dim * 2, out_dim)  # èŠ‚ç‚¹+é‚»å±…ç‰¹å¾concat

    def forward(self,
                node_features: torch.Tensor,  # (B, N, in_dim)
                edge_indices: torch.Tensor    # (B, 2, E)
                ) -> torch.Tensor:

        B, N, in_dim = node_features.shape
        device = node_features.device

        # ç®€åŒ–çš„å›¾å·ç§¯ï¼šå¯¹æ¯ä¸ªèŠ‚ç‚¹èšåˆé‚»å±…ç‰¹å¾
        output = []

        for b in range(B):
            batch_edges = edge_indices[b]  # (2, E)
            batch_nodes = node_features[b]  # (N, in_dim)

            # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ”¶é›†é‚»å±…
            node_outputs = []
            for n in range(N):
                # æ‰¾åˆ°ä»¥nä¸ºç›®æ ‡çš„è¾¹
                neighbor_indices = batch_edges[0][batch_edges[1] == n]

                if len(neighbor_indices) > 0:
                    # èšåˆé‚»å±…ç‰¹å¾
                    neighbor_features = batch_nodes[neighbor_indices]  # (num_neighbors, in_dim)
                    neighbor_agg = torch.mean(neighbor_features, dim=0)  # (in_dim,)
                else:
                    # æ²¡æœ‰é‚»å±…ï¼Œç”¨é›¶å¡«å……
                    neighbor_agg = torch.zeros_like(batch_nodes[n])

                # èŠ‚ç‚¹è‡ªèº«ç‰¹å¾ + é‚»å±…èšåˆç‰¹å¾
                combined = torch.cat([batch_nodes[n], neighbor_agg], dim=0)  # (2*in_dim,)
                node_outputs.append(combined)

            batch_output = torch.stack(node_outputs, dim=0)  # (N, 2*in_dim)
            output.append(batch_output)

        # åˆå¹¶æ‰¹æ¬¡
        batched_output = torch.stack(output, dim=0)  # (B, N, 2*in_dim)

        # çº¿æ€§å˜æ¢
        return self.linear(batched_output)  # (B, N, out_dim)

class AttentionPooling(nn.Module):
    """æ³¨æ„åŠ›æ± åŒ–ï¼šå°†(N, 32)èšåˆæˆ(1, 32)"""

    def __init__(self, input_dim: int = 32):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.Tanh(),
            nn.Linear(input_dim // 2, 1)
        )

    def forward(self, graph_tokens: torch.Tensor) -> torch.Tensor:  # (B, N, 32) -> (B, 1, 32)
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_scores = self.attention(graph_tokens)  # (B, N, 1)
        attention_weights = F.softmax(attention_scores, dim=1)  # (B, N, 1)

        # åŠ æƒèšåˆ
        robot_token = torch.sum(graph_tokens * attention_weights, dim=1, keepdim=True)  # (B, 1, 32)

        return robot_token

class NodeActionHead(nn.Module):
    """èŠ‚ç‚¹å¼åŠ¨ä½œå¤´

    ä¸ºæ¯ä¸ªå…³èŠ‚èŠ‚ç‚¹ç‹¬ç«‹é¢„æµ‹åŠ¨ä½œï¼Œç„¶åç»„åˆæˆå®Œæ•´è½¨è¿¹
    """

    def __init__(self,
                 vlm_feature_dim: int = 768,
                 node_token_dim: int = 32,
                 action_chunk_size: int = 16,
                 max_dof: int = 10):
        super().__init__()

        self.vlm_feature_dim = vlm_feature_dim
        self.node_token_dim = node_token_dim
        self.action_chunk_size = action_chunk_size
        self.max_dof = max_dof

        # ä¸ºæ¯ä¸ªå…³èŠ‚é¢„æµ‹åŠ¨ä½œçš„å¤´
        # è¾“å…¥: VLMç‰¹å¾ + èŠ‚ç‚¹token (æ®‹å·®è¿æ¥)
        self.joint_heads = nn.ModuleDict()

        # åŠ¨æ€åˆ›å»ºä¸åŒå½¢æ€çš„å¤´
        for num_segments in [2, 3, 4, 5]:
            for constraint_type in ["3DOF", "4DOF"]:
                num_joints = num_segments * 2  # æ¯æ®µä¸¤ä¸ªå‚æ•°(Î±, Î²)
                head_name = f"{num_segments}seg_{constraint_type}"

                self.joint_heads[head_name] = nn.ModuleList([
                    nn.Sequential(
                        nn.Linear(vlm_feature_dim + node_token_dim, 128),
                        nn.ReLU(),
                        nn.Linear(128, 64),
                        nn.ReLU(),
                        nn.Linear(64, action_chunk_size)  # é¢„æµ‹Hæ­¥çš„Î”q
                    ) for _ in range(num_joints)
                ])

    def forward(self,
                vlm_features: torch.Tensor,    # (B, vlm_dim)
                graph_tokens: torch.Tensor,    # (B, N, 32)
                robot_configs: List[str]       # batchçš„æœºå™¨äººé…ç½®
                ) -> torch.Tensor:

        B, N, _ = graph_tokens.shape
        device = graph_tokens.device

        # ä¸ºæ¯ä¸ªbatchæ ·æœ¬é¢„æµ‹åŠ¨ä½œ
        batch_actions = []

        for b in range(B):
            robot_config = robot_configs[b]

            # è§£ææœºå™¨äººé…ç½®
            if "2_segments" in robot_config:
                num_segments = 2
            elif "3_segments" in robot_config:
                num_segments = 3
            elif "4_segments" in robot_config:
                num_segments = 4
            elif "5_segments" in robot_config:
                num_segments = 5
            else:
                num_segments = 3  # é»˜è®¤

            constraint_type = "3DOF" if "3DOF" in robot_config else "4DOF"

            num_joints = num_segments * 2
            head_name = f"{num_segments}seg_{constraint_type}"

            # è·å–å¯¹åº”çš„åŠ¨ä½œå¤´
            if head_name not in self.joint_heads:
                # å¦‚æœæ²¡æœ‰å¯¹åº”çš„å¤´ï¼Œä½¿ç”¨é»˜è®¤çš„3seg_3DOF
                head_name = "3seg_3DOF"
                num_joints = 6

            joint_heads = self.joint_heads[head_name]

            # VLMç‰¹å¾
            batch_vlm_features = vlm_features[b:b+1].expand(num_joints, -1)  # (num_joints, vlm_dim)

            # å›¾tokenï¼ˆåªå–å‰num_jointsä¸ªï¼Œæˆ–è€…é‡å¤/æˆªæ–­ï¼‰
            if N >= num_joints:
                batch_graph_tokens = graph_tokens[b, :num_joints, :]  # (num_joints, 32)
            else:
                # ä¸å¤Ÿçš„è¯é‡å¤æœ€åä¸€ä¸ª
                batch_graph_tokens = graph_tokens[b]  # (N, 32)
                padding = batch_graph_tokens[-1:].expand(num_joints - N, -1)
                batch_graph_tokens = torch.cat([batch_graph_tokens, padding], dim=0)

            # é¢„æµ‹æ¯ä¸ªå…³èŠ‚çš„åŠ¨ä½œ
            joint_actions = []
            for j in range(num_joints):
                # ç»„åˆç‰¹å¾: VLM + èŠ‚ç‚¹token (æ®‹å·®)
                combined_features = torch.cat([
                    batch_vlm_features[j],
                    batch_graph_tokens[j]
                ], dim=0)  # (vlm_dim + 32,)

                # ä½¿ç”¨å¯¹åº”çš„å…³èŠ‚å¤´é¢„æµ‹
                delta_q = joint_heads[j](combined_features.unsqueeze(0))  # (1, action_chunk_size)
                joint_actions.append(delta_q)

            # æ‹¼æ¥æˆå®Œæ•´åŠ¨ä½œ (num_joints, action_chunk_size)
            sample_actions = torch.cat(joint_actions, dim=0)  # (num_joints, H)

            # å¡«å……åˆ°æœ€å¤§DoF
            if num_joints < self.max_dof:
                padding = torch.zeros(self.max_dof - num_joints, self.action_chunk_size, device=device)
                sample_actions = torch.cat([sample_actions, padding], dim=0)

            batch_actions.append(sample_actions)

        # ç»„åˆæ‰¹æ¬¡ (B, max_dof, action_chunk_size)
        batched_actions = torch.stack(batch_actions, dim=0)

        # è½¬ç½®ä¸º (B, action_chunk_size, max_dof) ç¬¦åˆFlow Matchingæ ¼å¼
        return batched_actions.transpose(1, 2)  # (B, H, DOF)

class SoftArmGraphVLA(nn.Module):
    """å®Œæ•´çš„è½¯ä½“è‡‚Graph-based VLAæ¨¡å‹

    æ¶æ„:
    1. å›¾åƒ+æ–‡æœ¬ â†’ VLM backbone
    2. æœºå™¨äººå›¾ â†’ GraphNN â†’ (NÃ—32 tokens)
    3. åŒè·¯èåˆ: attention pool (å…¨å±€) + æ®‹å·® (ç»†ç²’åº¦)
    4. èŠ‚ç‚¹å¼åŠ¨ä½œå¤´ â†’ æ‹¼è¡¨æ‰§è¡Œ â†’ Flow Matching
    """

    def __init__(self,
                 pretrained_checkpoint: str,
                 action_chunk_size: int = 16,
                 max_dof: int = 10,
                 graph_token_dim: int = 32):
        super().__init__()

        self.action_chunk_size = action_chunk_size
        self.max_dof = max_dof
        self.graph_token_dim = graph_token_dim

        # 1. åŠ è½½é¢„è®­ç»ƒçš„OpenPi VLM backbone
        self._load_vlm_backbone(pretrained_checkpoint)

        # 2. è½¯ä½“è‡‚å›¾ç¥ç»ç½‘ç»œ
        self.graph_nn = SoftArmGraphNN(
            input_dim=19,
            output_dim=graph_token_dim
        )

        # 3. åŒè·¯èåˆ
        self.attention_pooling = AttentionPooling(graph_token_dim)

        # 4. èŠ‚ç‚¹å¼åŠ¨ä½œå¤´
        self.action_head = NodeActionHead(
            vlm_feature_dim=768,  # å‡è®¾OpenPiçš„ç‰¹å¾ç»´åº¦
            node_token_dim=graph_token_dim,
            action_chunk_size=action_chunk_size,
            max_dof=max_dof
        )

        # 5. Flow Matchingç»„ä»¶ (ç®€åŒ–ç‰ˆ)
        self.flow_matching = SimpleFlowMatching(
            action_dim=max_dof,
            action_chunk_size=action_chunk_size
        )

    def _load_vlm_backbone(self, checkpoint_path: str):
        """åŠ è½½OpenPi VLM backbone"""
        try:
            # åŠ è½½OpenPié¢„è®­ç»ƒæ¨¡å‹
            config = checkpoint_to_config(checkpoint_path)
            self.vlm_backbone = pi0_5.Pi05(config)

            # åŠ è½½æƒé‡
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            self.vlm_backbone.load_state_dict(checkpoint['model'], strict=False)

            print(f"âœ… æˆåŠŸåŠ è½½OpenPi VLM backbone: {checkpoint_path}")

        except Exception as e:
            print(f"âš ï¸ OpenPiåŠ è½½å¤±è´¥: {e}")
            print("  ä½¿ç”¨ç®€åŒ–çš„VLM backbone")

            # ç®€åŒ–çš„VLM backbone
            self.vlm_backbone = SimplifiedVLMBackbone()

    def forward(self,
                images: torch.Tensor,           # (B, C, H, W)
                instructions: List[str],        # Bä¸ªæŒ‡ä»¤
                node_features: torch.Tensor,   # (B, N, 19)
                edge_indices: torch.Tensor,    # (B, 2, E)
                robot_configs: List[str],      # Bä¸ªæœºå™¨äººé…ç½®
                target_actions: Optional[torch.Tensor] = None,  # (B, H, DOF) ç”¨äºè®­ç»ƒ
                timesteps: Optional[torch.Tensor] = None        # Flow Matchingæ—¶é—´æ­¥
                ) -> Dict[str, torch.Tensor]:

        B = images.shape[0]

        # 1. VLMç¼–ç  (å›¾åƒ+æ–‡æœ¬)
        try:
            vlm_features = self.vlm_backbone.encode(images, instructions)  # (B, 768)
        except:
            # å›é€€åˆ°ç®€åŒ–ç¼–ç 
            vlm_features = self.vlm_backbone(images, instructions)

        # 2. å›¾ç»“æ„ç¼–ç 
        graph_tokens = self.graph_nn(node_features, edge_indices, B)  # (B, N, 32)

        # 3. åŒè·¯èåˆ
        # å…¨å±€è·¯: attention pooling â†’ è¿›backbone
        robot_token = self.attention_pooling(graph_tokens)  # (B, 1, 32)

        # å°†robot_tokenèå…¥VLMç‰¹å¾ (ç®€åŒ–èåˆ)
        if vlm_features.dim() == 2:  # (B, 768)
            robot_global = robot_token.squeeze(1)  # (B, 32)
            # çº¿æ€§æŠ•å½±åˆ°VLMç»´åº¦
            if not hasattr(self, 'robot_proj'):
                self.robot_proj = nn.Linear(32, vlm_features.shape[1], device=vlm_features.device)
            robot_proj = self.robot_proj(robot_global)  # (B, 768)
            enhanced_vlm_features = vlm_features + robot_proj  # æ®‹å·®èåˆ
        else:
            enhanced_vlm_features = vlm_features

        # 4. èŠ‚ç‚¹å¼åŠ¨ä½œé¢„æµ‹
        # ç»†ç²’åº¦è·¯: æœªæ± åŒ–çš„graph_tokensä½œä¸ºæ¡ä»¶
        predicted_actions = self.action_head(
            enhanced_vlm_features,  # (B, 768)
            graph_tokens,           # (B, N, 32) æ®‹å·®è·¯å¾„
            robot_configs           # æœºå™¨äººé…ç½®
        )  # (B, H, DOF)

        # 5. Flow Matching (è®­ç»ƒæ—¶)
        results = {
            'predicted_actions': predicted_actions,
            'graph_tokens': graph_tokens,
            'robot_token': robot_token,
            'vlm_features': enhanced_vlm_features,
        }

        if target_actions is not None:
            # è®­ç»ƒæ¨¡å¼: è®¡ç®—Flow MatchingæŸå¤±
            flow_loss = self.flow_matching(
                predicted_actions,
                target_actions,
                timesteps
            )
            results['flow_loss'] = flow_loss

        return results

class SimplifiedVLMBackbone(nn.Module):
    """ç®€åŒ–çš„VLM backboneï¼Œç”¨äºæµ‹è¯•"""

    def __init__(self):
        super().__init__()

        # å›¾åƒç¼–ç å™¨ (ç®€åŒ–çš„CNN)
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 32, 7, 2, 3),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((7, 7)),
            nn.Flatten(),
            nn.Linear(32 * 7 * 7, 512),
            nn.ReLU(),
            nn.Linear(512, 768)
        )

        # æ–‡æœ¬ç¼–ç å™¨ (ç®€åŒ–çš„tokenåµŒå…¥)
        self.text_encoder = nn.Sequential(
            nn.Embedding(10000, 256),  # è¯æ±‡è¡¨å¤§å°
            nn.Linear(256, 768)
        )

    def forward(self, images: torch.Tensor, instructions: List[str]) -> torch.Tensor:
        # å›¾åƒç‰¹å¾
        image_features = self.image_encoder(images)  # (B, 768)

        # æ–‡æœ¬ç‰¹å¾ (ç®€åŒ–: ç”¨æŒ‡ä»¤é•¿åº¦ä½œä¸ºç‰¹å¾)
        text_lengths = torch.tensor([len(inst) for inst in instructions],
                                   device=images.device, dtype=torch.long)
        text_lengths = torch.clamp(text_lengths, 0, 9999)  # é™åˆ¶èŒƒå›´
        text_features = self.text_encoder[1](
            self.text_encoder[0](text_lengths).mean(dim=1)
        )  # (B, 768)

        # ç®€å•èåˆ
        return image_features + text_features

class SimpleFlowMatching(nn.Module):
    """ç®€åŒ–çš„Flow Matchingå®ç°"""

    def __init__(self, action_dim: int, action_chunk_size: int):
        super().__init__()
        self.action_dim = action_dim
        self.action_chunk_size = action_chunk_size

    def forward(self,
                predicted_actions: torch.Tensor,    # (B, H, DOF)
                target_actions: torch.Tensor,       # (B, H, DOF)
                timesteps: Optional[torch.Tensor] = None
                ) -> torch.Tensor:

        # ç®€åŒ–çš„MSEæŸå¤± (çœŸæ­£çš„Flow Matchingæ›´å¤æ‚)
        # è¿™é‡Œåªæ˜¯å ä½ï¼ŒçœŸæ­£å®ç°éœ€è¦å™ªå£°è°ƒåº¦ã€æ—¶é—´æ­¥ç­‰

        B, H, DOF = predicted_actions.shape

        # åªè®¡ç®—æœ‰æ•ˆDoFçš„æŸå¤±
        # å‡è®¾target_actionsä¸­å¡«å……çš„éƒ¨åˆ†ä¸º0
        valid_mask = (target_actions.abs().sum(dim=1) > 0).float()  # (B, DOF)
        valid_mask = valid_mask.unsqueeze(1).expand(-1, H, -1)  # (B, H, DOF)

        # åŠ æƒMSEæŸå¤±
        mse_loss = F.mse_loss(predicted_actions, target_actions, reduction='none')  # (B, H, DOF)
        weighted_loss = (mse_loss * valid_mask).sum() / (valid_mask.sum() + 1e-8)

        return weighted_loss

def create_soft_arm_graph_vla(config: Dict[str, Any]) -> SoftArmGraphVLA:
    """åˆ›å»ºè½¯ä½“è‡‚Graph VLAæ¨¡å‹"""

    model = SoftArmGraphVLA(
        pretrained_checkpoint=config.get('pretrained_checkpoint', '~/.cache/openpi/checkpoints/pi05_droid'),
        action_chunk_size=config.get('action_chunk_size', 16),
        max_dof=config.get('max_dof', 10),
        graph_token_dim=config.get('graph_token_dim', 32)
    )

    return model

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹
    print("ğŸ§ª æµ‹è¯•Graph-based VLAæ¨¡å‹...")

    # æ¨¡æ‹Ÿæ•°æ®
    B, H, W = 2, 224, 224
    N, E = 6, 14  # 3æ®µè½¯ä½“è‡‚çš„å›¾ç»“æ„

    # è¾“å…¥æ•°æ®
    images = torch.randn(B, 3, H, W)
    instructions = ["Pick up the red cube", "Move to target position"]
    node_features = torch.randn(B, N, 19)  # å›¾èŠ‚ç‚¹ç‰¹å¾
    edge_indices = torch.randint(0, N, (B, 2, E))  # è¾¹ç´¢å¼•
    robot_configs = ["3_segments_3DOF_default", "3_segments_3DOF_default"]
    target_actions = torch.randn(B, 16, 10)  # ç›®æ ‡åŠ¨ä½œ

    # åˆ›å»ºæ¨¡å‹
    config = {
        'action_chunk_size': 16,
        'max_dof': 10,
        'graph_token_dim': 32
    }

    model = create_soft_arm_graph_vla(config)

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        results = model(
            images=images,
            instructions=instructions,
            node_features=node_features,
            edge_indices=edge_indices,
            robot_configs=robot_configs,
            target_actions=target_actions
        )

    print("âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
    print(f"   é¢„æµ‹åŠ¨ä½œå½¢çŠ¶: {results['predicted_actions'].shape}")
    print(f"   å›¾tokenå½¢çŠ¶: {results['graph_tokens'].shape}")
    print(f"   æœºå™¨äººtokenå½¢çŠ¶: {results['robot_token'].shape}")
    print(f"   FlowæŸå¤±: {results['flow_loss'].item():.6f}")