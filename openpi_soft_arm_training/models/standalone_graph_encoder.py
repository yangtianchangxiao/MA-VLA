#!/usr/bin/env python3
"""
ç‹¬ç«‹å›¾ç¼–ç å™¨ - ä¸ä¾èµ–OpenPiå¤æ‚ç¯å¢ƒ
ç”¨äºå¿«é€Ÿæµ‹è¯•å’ŒéªŒè¯è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional

class StandaloneGraphEncoder(nn.Module):
    """ç‹¬ç«‹å›¾ç¼–ç å™¨ - ä¸OpenPiç¯å¢ƒè§£è€¦"""

    def __init__(self, input_dim: int = 19, output_dim: int = 32):
        super().__init__()

        # èŠ‚ç‚¹ç¼–ç 
        self.node_encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )

        # ç®€å•è‡ªæ³¨æ„åŠ›
        self.attention = nn.MultiheadAttention(
            embed_dim=output_dim,
            num_heads=4,
            batch_first=True
        )

        self.norm = nn.LayerNorm(output_dim)

    def forward(self, graph_data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            graph_data: {
                'node_features': (B, N, 19),
                'num_nodes': (B,) - æœ‰æ•ˆèŠ‚ç‚¹æ•°
            }
        Returns:
            graph_embedding: (B, 32) - å…¨å±€å›¾è¡¨ç¤º
        """
        node_features = graph_data['node_features']  # (B, N, 19)
        num_nodes = graph_data.get('num_nodes', None)

        # èŠ‚ç‚¹ç¼–ç 
        node_embeddings = self.node_encoder(node_features)  # (B, N, 32)

        # è‡ªæ³¨æ„åŠ›
        attended, _ = self.attention(
            node_embeddings, node_embeddings, node_embeddings
        )  # (B, N, 32)

        # æ®‹å·® + å½’ä¸€åŒ–
        node_embeddings = self.norm(attended + node_embeddings)

        # å…¨å±€æ± åŒ– (è€ƒè™‘æœ‰æ•ˆèŠ‚ç‚¹)
        if num_nodes is not None:
            # åˆ›å»ºmask
            B, N = node_embeddings.shape[:2]
            mask = torch.arange(N, device=node_embeddings.device)[None, :] < num_nodes[:, None]
            mask = mask.float().unsqueeze(-1)  # (B, N, 1)

            # åŠ æƒå¹³å‡
            global_embedding = (node_embeddings * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)
        else:
            # ç®€å•å¹³å‡æ± åŒ–
            global_embedding = node_embeddings.mean(dim=1)  # (B, 32)

        return global_embedding

class MockPI0Model(nn.Module):
    """æ¨¡æ‹ŸPI0æ¨¡å‹ - ç”¨äºæµ‹è¯•è®­ç»ƒæµç¨‹"""

    def __init__(self, action_dim: int = 10, action_horizon: int = 16, enable_graph: bool = True):
        super().__init__()

        self.action_dim = action_dim
        self.action_horizon = action_horizon
        self.enable_graph = enable_graph

        # å›¾ç¼–ç å™¨
        if enable_graph:
            self.graph_encoder = StandaloneGraphEncoder(19, 32)

        # ç®€åŒ–çš„è§†è§‰ç¼–ç å™¨
        self.visual_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(7),
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 512)
        )

        # æ–‡æœ¬ç¼–ç å™¨ (ç®€åŒ–)
        self.text_encoder = nn.Linear(384, 128)  # å‡è®¾æ–‡æœ¬ç‰¹å¾ç»´åº¦

        # èåˆå±‚
        feature_dim = 512 + 128  # visual + text
        if enable_graph:
            feature_dim += 32  # + graph

        # åŠ¨ä½œé¢„æµ‹å¤´
        self.action_head = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.Linear(512, action_horizon * action_dim)
        )

        print(f"âœ… Mock PI0æ¨¡å‹åˆ›å»ºå®Œæˆ (å›¾æ”¯æŒ: {enable_graph})")

    def forward(self, observation, actions, graph_data=None):
        """å‰å‘ä¼ æ’­"""
        batch_size = actions.shape[0]

        # å¤„ç†è§‚å¯Ÿ
        features = []

        # è§†è§‰ç‰¹å¾
        if 'image' in observation and 'camera_0' in observation['image']:
            visual_feat = self.visual_encoder(observation['image']['camera_0'])
            features.append(visual_feat)
        else:
            # å ä½è§†è§‰ç‰¹å¾
            features.append(torch.zeros(batch_size, 512, device=actions.device))

        # æ–‡æœ¬ç‰¹å¾ (ç®€åŒ–å¤„ç†)
        if 'instruction' in observation:
            # ç®€å•çš„æ–‡æœ¬embedding (å®é™…åº”è¯¥ç”¨transformer)
            text_feat = torch.randn(batch_size, 128, device=actions.device)
            features.append(text_feat)
        else:
            features.append(torch.zeros(batch_size, 128, device=actions.device))

        # å›¾ç‰¹å¾
        if self.enable_graph and graph_data is not None:
            graph_feat = self.graph_encoder(graph_data)
            features.append(graph_feat)
        elif self.enable_graph:
            features.append(torch.zeros(batch_size, 32, device=actions.device))

        # ç‰¹å¾èåˆ
        fused_features = torch.cat(features, dim=1)

        # åŠ¨ä½œé¢„æµ‹
        predicted_actions = self.action_head(fused_features)
        predicted_actions = predicted_actions.view(batch_size, self.action_horizon, self.action_dim)

        # è®¡ç®—æŸå¤± (ç®€å•MSE)
        loss = F.mse_loss(predicted_actions, actions)

        return loss

def test_standalone_components():
    """æµ‹è¯•ç‹¬ç«‹ç»„ä»¶"""
    print("ğŸ§ª æµ‹è¯•ç‹¬ç«‹å›¾ç¼–ç å™¨...")

    # æµ‹è¯•å›¾ç¼–ç å™¨
    encoder = StandaloneGraphEncoder(19, 32)
    graph_data = {
        'node_features': torch.randn(2, 10, 19),
        'num_nodes': torch.tensor([6, 8])
    }

    with torch.no_grad():
        output = encoder(graph_data)

    print(f"âœ… å›¾ç¼–ç å™¨æµ‹è¯•æˆåŠŸ: {graph_data['node_features'].shape} â†’ {output.shape}")

    # æµ‹è¯•Mockæ¨¡å‹
    print("\nğŸ§ª æµ‹è¯•Mock PI0æ¨¡å‹...")

    model = MockPI0Model(action_dim=10, action_horizon=16, enable_graph=True)

    # æ¨¡æ‹Ÿæ•°æ®
    observation = {
        'image': {'camera_0': torch.randn(2, 3, 224, 224)},
        'instruction': ['task 1', 'task 2']
    }
    actions = torch.randn(2, 16, 10)

    with torch.no_grad():
        loss = model(observation, actions, graph_data)

    print(f"âœ… Mockæ¨¡å‹æµ‹è¯•æˆåŠŸ: loss = {loss.item():.4f}")

    # æµ‹è¯•GPU
    if torch.cuda.is_available():
        print("\nğŸ§ª æµ‹è¯•GPUå…¼å®¹æ€§...")
        device = torch.device('cuda:0')

        model = model.to(device)
        observation = {k: v.to(device) if isinstance(v, torch.Tensor) else
                      {k2: v2.to(device) if isinstance(v2, torch.Tensor) else v2 for k2, v2 in v.items()}
                      if isinstance(v, dict) else v
                      for k, v in observation.items()}
        actions = actions.to(device)
        graph_data = {k: v.to(device) for k, v in graph_data.items()}

        with torch.no_grad():
            loss = model(observation, actions, graph_data)

        print(f"âœ… GPUæµ‹è¯•æˆåŠŸ: loss = {loss.item():.4f}, device = {loss.device}")

    return True

if __name__ == "__main__":
    test_standalone_components()
    print("\nğŸ‰ ç‹¬ç«‹ç»„ä»¶æµ‹è¯•å®Œæˆ!")