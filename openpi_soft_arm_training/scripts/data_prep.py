#!/usr/bin/env python3
"""
è½¯ä½“è‡‚æ•°æ®é¢„å¤„ç†è„šæœ¬
å°†åˆ†æ•£çš„æ•°æ®æ•´ç†æˆç»Ÿä¸€æ ¼å¼ï¼Œä¾¿äºè®­ç»ƒ
"""

import os
import sys
import json
import h5py
import pickle
import yaml
from pathlib import Path
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from tqdm import tqdm
import cv2

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))
sys.path.append('/home/cx/AET_FOR_RL/vla')

def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def ensure_dir(path: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)

class SoftArmDataProcessor:
    """è½¯ä½“è‡‚æ•°æ®å¤„ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.raw_paths = config['raw_data']
        self.processed_paths = config['processed_data']

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        root_dir = self.processed_paths['root']
        ensure_dir(root_dir)
        ensure_dir(os.path.join(root_dir, 'processed'))
        ensure_dir(os.path.join(root_dir, 'processed', 'image_cache'))

        self.episodes = []
        self.failed_episodes = []

    def process_all_data(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®"""
        print("ğŸš€ å¼€å§‹è½¯ä½“è‡‚æ•°æ®é¢„å¤„ç†")
        print("=" * 50)

        # 1. å¤„ç†è½¯ä½“è‡‚å…³èŠ‚æ•°æ®
        self._process_soft_arm_data()

        # 2. éªŒè¯å›¾åƒæ•°æ®
        self._verify_image_data()

        # 3. å¤„ç†æœºå™¨äººå›¾æ•°æ®
        self._process_robot_graphs()

        # 4. åˆ›å»ºè®­ç»ƒæ•°æ®åˆ†å‰²
        self._create_data_splits()

        # 5. ä¿å­˜ç»Ÿä¸€æ•°æ®
        self._save_unified_data()

        # 6. ç”Ÿæˆæ•°æ®ç»Ÿè®¡
        self._generate_statistics()

    def _process_soft_arm_data(self):
        """å¤„ç†è½¯ä½“è‡‚åˆæˆæ•°æ®"""
        print("\nğŸ“Š å¤„ç†è½¯ä½“è‡‚å…³èŠ‚æ•°æ®...")

        # å¤„ç†3DOFæ•°æ®
        dof3_dir = self.raw_paths['soft_arm_3dof']
        if os.path.exists(dof3_dir):
            self._process_morphology_data(dof3_dir, "3DOF")

        # å¤„ç†4DOFæ•°æ®
        dof4_dir = self.raw_paths['soft_arm_4dof']
        if os.path.exists(dof4_dir):
            self._process_morphology_data(dof4_dir, "4DOF")

        print(f"âœ… æ€»å…±å¤„ç†äº† {len(self.episodes)} ä¸ªepisode")
        if self.failed_episodes:
            print(f"âš ï¸ å¤±è´¥äº† {len(self.failed_episodes)} ä¸ªepisode")

    def _process_morphology_data(self, data_dir: str, constraint_type: str):
        """å¤„ç†ç‰¹å®šå½¢æ€çš„æ•°æ®"""

        for episode_dir in tqdm(os.listdir(data_dir), desc=f"Processing {constraint_type}"):
            if not episode_dir.startswith('episode_'):
                continue

            episode_path = os.path.join(data_dir, episode_dir)
            if not os.path.isdir(episode_path):
                continue

            episode_id = episode_dir
            original_episode_id = episode_id.split('episode_')[1].split('_')[0]

            # å¤„ç†æ¯ä¸ªæ®µæ•°é…ç½®
            for segment_dir in os.listdir(episode_path):
                if not segment_dir.endswith('_segments'):
                    continue

                segment_path = os.path.join(episode_path, segment_dir)
                if not os.path.isdir(segment_path):
                    continue

                try:
                    # åŠ è½½å…³èŠ‚è½¨è¿¹
                    trajectory_file = os.path.join(segment_path, 'joint_trajectory.npz')
                    config_file = os.path.join(segment_path, 'config.json')

                    if not os.path.exists(trajectory_file):
                        continue

                    # åŠ è½½æ•°æ®
                    traj_data = np.load(trajectory_file)

                    # åŸºæœ¬ä¿¡æ¯
                    num_segments = int(segment_dir.split('_')[0])
                    robot_config = f"{segment_dir}_{constraint_type}"

                    # åŠ è½½é…ç½®
                    task_description = f"Complete manipulation task using {constraint_type} soft continuum arm with {segment_dir}"
                    if os.path.exists(config_file):
                        with open(config_file, 'r') as f:
                            config_data = json.load(f)
                            if 'task_description' in config_data:
                                task_description = str(config_data['task_description'])

                    # æå–è½¨è¿¹æ•°æ®
                    joint_positions = traj_data['joint_positions']  # (N, action_dim)
                    ee_positions = traj_data['end_effector_positions']  # (N, 3)
                    ee_orientations = traj_data['end_effector_orientations']  # (N, 3)
                    timestamps = traj_data['timestamps']
                    success_mask = traj_data['success_mask']

                    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                    if len(joint_positions) != len(ee_positions) or len(joint_positions) == 0:
                        self.failed_episodes.append(f"{episode_id}/{segment_dir}: æ•°æ®é•¿åº¦ä¸åŒ¹é…")
                        continue

                    # åˆ›å»ºepisodeè®°å½•
                    episode_record = {
                        'episode_id': episode_id,
                        'original_episode_id': original_episode_id,
                        'robot_config': robot_config,
                        'constraint_type': constraint_type,
                        'num_segments': num_segments,
                        'action_dim': joint_positions.shape[1],
                        'sequence_length': len(joint_positions),
                        'task_description': task_description,

                        # æ•°æ®è·¯å¾„ï¼ˆç›¸å¯¹äºåŸå§‹æ•°æ®ï¼‰
                        'trajectory_file': trajectory_file,
                        'config_file': config_file if os.path.exists(config_file) else None,

                        # æ•°æ®æ‘˜è¦
                        'joint_positions_shape': joint_positions.shape,
                        'ee_positions_shape': ee_positions.shape,
                        'success_rate': float(np.mean(success_mask)) if len(success_mask) > 0 else 0.0,
                        'temporal_smoothness': float(traj_data.get('temporal_smoothness', 0.0)),
                        'position_accuracy': float(traj_data.get('position_accuracy', 0.0)),

                        # ç”¨äºå¿«é€Ÿè®¿é—®çš„ç¼“å­˜
                        'has_valid_images': False,  # åç»­å¡«å……
                        'robot_graph_path': None,   # åç»­å¡«å……
                    }

                    self.episodes.append(episode_record)

                except Exception as e:
                    self.failed_episodes.append(f"{episode_id}/{segment_dir}: {str(e)}")
                    continue

    def _verify_image_data(self):
        """éªŒè¯å›¾åƒæ•°æ®å¯ç”¨æ€§"""
        print("\nğŸ–¼ï¸ éªŒè¯å›¾åƒæ•°æ®...")

        image_root = self.raw_paths['droid_images']
        valid_images = 0
        total_episodes = 0

        for episode in tqdm(self.episodes, desc="Checking images"):
            original_id = episode['original_episode_id']
            episode_image_dir = os.path.join(image_root, f"episode_{original_id}")

            total_episodes += 1

            if os.path.exists(episode_image_dir):
                # æ£€æŸ¥ç›¸æœºè§†è§’
                camera_views = ['exterior_image_1_left', 'exterior_image_2_left', 'wrist_image_left']
                found_camera = False

                for camera in camera_views:
                    camera_dir = os.path.join(episode_image_dir, camera)
                    if os.path.exists(camera_dir):
                        images = [f for f in os.listdir(camera_dir) if f.endswith('.jpg')]
                        if len(images) > 0:
                            episode['image_camera'] = camera
                            episode['image_count'] = len(images)
                            episode['has_valid_images'] = True
                            found_camera = True
                            valid_images += 1
                            break

                if not found_camera:
                    episode['has_valid_images'] = False
            else:
                episode['has_valid_images'] = False

        print(f"âœ… å›¾åƒæ•°æ®: {valid_images}/{total_episodes} episodesæœ‰æ•ˆ")

    def _process_robot_graphs(self):
        """å¤„ç†æœºå™¨äººå›¾æ•°æ®"""
        print("\nğŸ•¸ï¸ å¤„ç†æœºå™¨äººå›¾æ•°æ®...")

        graph_root = self.raw_paths['robot_graphs']
        graph_cache = {}
        found_graphs = 0

        for episode in tqdm(self.episodes, desc="Loading robot graphs"):
            robot_config = episode['robot_config']
            num_segments = episode['num_segments']
            constraint_type = episode['constraint_type']

            # æŸ¥æ‰¾å¯¹åº”çš„å›¾æ–‡ä»¶
            # è½¯ä½“è‡‚å›¾æ–‡ä»¶å‘½åä¸º soft_arm_Nsegments_constraint.npz
            graph_file = os.path.join(graph_root, f"soft_arm_{num_segments}segments_{constraint_type}.npz")

            if os.path.exists(graph_file):
                episode['robot_graph_path'] = graph_file

                # åŠ è½½åˆ°ç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
                if graph_file not in graph_cache:
                    try:
                        graph_data = np.load(graph_file)
                        graph_cache[graph_file] = {
                            'node_features': graph_data['node_features'],  # (N, 19)
                            'edge_indices': graph_data['edge_indices'],    # (2, E)
                            'num_nodes': int(graph_data['num_nodes']),
                            'num_edges': int(graph_data['num_edges']),
                        }
                        found_graphs += 1
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½å›¾å¤±è´¥: {graph_file}, é”™è¯¯: {e}")
                        episode['robot_graph_path'] = None
            else:
                episode['robot_graph_path'] = None

        print(f"âœ… æœºå™¨äººå›¾: æ‰¾åˆ° {len(graph_cache)} ä¸ªå”¯ä¸€å›¾æ–‡ä»¶")

        # ä¿å­˜å›¾ç¼“å­˜
        graph_cache_path = os.path.join(self.processed_paths['root'], 'processed', 'graph_cache.pkl')
        with open(graph_cache_path, 'wb') as f:
            pickle.dump(graph_cache, f)
        print(f"âœ… å›¾ç¼“å­˜å·²ä¿å­˜: {graph_cache_path}")

    def _create_data_splits(self):
        """åˆ›å»ºè®­ç»ƒ/éªŒè¯æ•°æ®åˆ†å‰²"""
        print("\nğŸ“‚ åˆ›å»ºæ•°æ®åˆ†å‰²...")

        # åªä½¿ç”¨æœ‰æ•ˆæ•°æ®
        valid_episodes = [ep for ep in self.episodes
                         if ep['has_valid_images'] and ep['robot_graph_path'] is not None]

        print(f"ğŸ“Š æœ‰æ•ˆepisodes: {len(valid_episodes)}/{len(self.episodes)}")

        # æŒ‰åŸå§‹episodeåˆ†å‰²ï¼Œé¿å…æ•°æ®æ³„éœ²
        unique_original_ids = list(set(ep['original_episode_id'] for ep in valid_episodes))
        unique_original_ids.sort()

        # 80-20åˆ†å‰²
        split_idx = int(len(unique_original_ids) * 0.8)
        train_original_ids = set(unique_original_ids[:split_idx])
        val_original_ids = set(unique_original_ids[split_idx:])

        train_episodes = []
        val_episodes = []

        for ep in valid_episodes:
            if ep['original_episode_id'] in train_original_ids:
                train_episodes.append(ep)
            else:
                val_episodes.append(ep)

        print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_episodes)} episodes")
        print(f"ğŸ“Š éªŒè¯é›†: {len(val_episodes)} episodes")

        # ä¿å­˜åˆ†å‰²ä¿¡æ¯
        train_split_path = os.path.join(self.processed_paths['root'], 'processed', 'train_episodes.json')
        val_split_path = os.path.join(self.processed_paths['root'], 'processed', 'val_episodes.json')

        with open(train_split_path, 'w') as f:
            json.dump(train_episodes, f, indent=2)

        with open(val_split_path, 'w') as f:
            json.dump(val_episodes, f, indent=2)

        print(f"âœ… è®­ç»ƒåˆ†å‰²å·²ä¿å­˜: {train_split_path}")
        print(f"âœ… éªŒè¯åˆ†å‰²å·²ä¿å­˜: {val_split_path}")

        self.train_episodes = train_episodes
        self.val_episodes = val_episodes

    def _save_unified_data(self):
        """ä¿å­˜ç»Ÿä¸€çš„HDF5æ ¼å¼æ•°æ®"""
        print("\nğŸ’¾ ä¿å­˜ç»Ÿä¸€æ•°æ®æ ¼å¼...")

        h5_path = os.path.join(self.processed_paths['root'], 'processed', 'unified_episodes.h5')

        with h5py.File(h5_path, 'w') as h5f:
            # åˆ›å»ºç»„
            train_group = h5f.create_group('train')
            val_group = h5f.create_group('val')

            # ä¿å­˜è®­ç»ƒæ•°æ®
            self._save_episodes_to_h5(train_group, self.train_episodes, 'train')

            # ä¿å­˜éªŒè¯æ•°æ®
            self._save_episodes_to_h5(val_group, self.val_episodes, 'val')

        print(f"âœ… ç»Ÿä¸€æ•°æ®å·²ä¿å­˜: {h5_path}")

    def _save_episodes_to_h5(self, group: h5py.Group, episodes: List[Dict], split_name: str):
        """ä¿å­˜episodesåˆ°HDF5ç»„"""

        for i, episode in enumerate(tqdm(episodes, desc=f"Saving {split_name} data")):
            ep_group = group.create_group(f"episode_{i}")

            # ä¿å­˜å…ƒæ•°æ®
            for key, value in episode.items():
                if isinstance(value, (str, int, float, bool)):
                    ep_group.attrs[key] = value
                elif isinstance(value, (list, tuple)) and len(value) > 0:
                    ep_group.attrs[key] = json.dumps(value)

            # åŠ è½½å¹¶ä¿å­˜è½¨è¿¹æ•°æ®
            try:
                traj_data = np.load(episode['trajectory_file'])
                ep_group.create_dataset('joint_positions', data=traj_data['joint_positions'])
                ep_group.create_dataset('ee_positions', data=traj_data['end_effector_positions'])
                ep_group.create_dataset('ee_orientations', data=traj_data['end_effector_orientations'])
                ep_group.create_dataset('timestamps', data=traj_data['timestamps'])
                ep_group.create_dataset('success_mask', data=traj_data['success_mask'])
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜è½¨è¿¹æ•°æ®å¤±è´¥: {episode['episode_id']}, é”™è¯¯: {e}")

    def _generate_statistics(self):
        """ç”Ÿæˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“ˆ ç”Ÿæˆæ•°æ®ç»Ÿè®¡...")

        stats = {
            'total_episodes': len(self.episodes),
            'valid_episodes': len(self.train_episodes) + len(self.val_episodes),
            'train_episodes': len(self.train_episodes),
            'val_episodes': len(self.val_episodes),
            'failed_episodes': len(self.failed_episodes),

            'constraint_types': {},
            'segment_counts': {},
            'action_dimensions': {},
            'sequence_lengths': [],
            'success_rates': [],
        }

        # ç»Ÿè®¡å„ç§ç»´åº¦
        valid_episodes = self.train_episodes + self.val_episodes

        for ep in valid_episodes:
            # çº¦æŸç±»å‹
            constraint = ep['constraint_type']
            stats['constraint_types'][constraint] = stats['constraint_types'].get(constraint, 0) + 1

            # æ®µæ•°
            segments = ep['num_segments']
            stats['segment_counts'][segments] = stats['segment_counts'].get(segments, 0) + 1

            # åŠ¨ä½œç»´åº¦
            action_dim = ep['action_dim']
            stats['action_dimensions'][action_dim] = stats['action_dimensions'].get(action_dim, 0) + 1

            # åºåˆ—é•¿åº¦å’ŒæˆåŠŸç‡
            stats['sequence_lengths'].append(ep['sequence_length'])
            stats['success_rates'].append(ep['success_rate'])

        # è®¡ç®—ç»Ÿè®¡é‡
        if stats['sequence_lengths']:
            stats['avg_sequence_length'] = np.mean(stats['sequence_lengths'])
            stats['avg_success_rate'] = np.mean(stats['success_rates'])

        # ä¿å­˜ç»Ÿè®¡
        stats_path = os.path.join(self.processed_paths['root'], 'processed', 'statistics.json')
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2, default=str)

        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")

        # æ‰“å°å…³é”®ç»Ÿè®¡
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
        print(f"  æ€»episodes: {stats['total_episodes']}")
        print(f"  æœ‰æ•ˆepisodes: {stats['valid_episodes']}")
        print(f"  è®­ç»ƒepisodes: {stats['train_episodes']}")
        print(f"  éªŒè¯episodes: {stats['val_episodes']}")
        print(f"  å¹³å‡åºåˆ—é•¿åº¦: {stats.get('avg_sequence_length', 0):.1f}")
        print(f"  å¹³å‡æˆåŠŸç‡: {stats.get('avg_success_rate', 0):.3f}")
        print(f"  çº¦æŸç±»å‹åˆ†å¸ƒ: {stats['constraint_types']}")
        print(f"  æ®µæ•°åˆ†å¸ƒ: {stats['segment_counts']}")

def main():
    """ä¸»å‡½æ•°"""

    # åŠ è½½é…ç½®
    config_path = os.path.join(PROJECT_ROOT, 'configs', 'data_paths.yaml')
    config = load_config(config_path)

    # åˆ›å»ºå¤„ç†å™¨
    processor = SoftArmDataProcessor(config)

    # å¼€å§‹å¤„ç†
    processor.process_all_data()

    print("\nğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆ!")
    print("=" * 50)
    print("ä¸‹ä¸€æ­¥:")
    print("1. æ£€æŸ¥ç”Ÿæˆçš„ç»Ÿè®¡ä¿¡æ¯")
    print("2. è¿è¡Œè®­ç»ƒè„šæœ¬æµ‹è¯•")
    print("3. å¼€å§‹æ¨¡å‹è®­ç»ƒ")

if __name__ == "__main__":
    main()