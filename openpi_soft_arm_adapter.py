#!/usr/bin/env python3
"""
OpenPiè½¯ä½“è‡‚æ•°æ®é€‚é…å™¨
å°†æˆ‘ä»¬çš„è½¯ä½“è‡‚åˆæˆæ•°æ®é€‚é…ä¸ºOpenPiå¯è®­ç»ƒçš„æ ¼å¼ï¼Œæ”¯æŒ8å¡è®­ç»ƒ
"""

import os
import json
import numpy as np
import torch
from pathlib import Path
from PIL import Image
from typing import Dict, List, Iterator, Optional
import pandas as pd

class SoftArmOpenPiDataset:
    """è½¯ä½“è‡‚æ•°æ®é›†ï¼Œé€‚é…OpenPiè®­ç»ƒæ¡†æ¶"""

    def __init__(self,
                 soft_arm_data_dir: str = "/home/cx/AET_FOR_RL/vla/synthesized_data/soft_arm_4dof_synthesis",
                 droid_images_dir: str = "/home/cx/AET_FOR_RL/vla/valid_original_data/droid_100/extracted_images",
                 droid_parquet_path: str = "/home/cx/AET_FOR_RL/vla/converted_data/droid_100_fixed/data/chunk-000/file-000.parquet",
                 task_descriptions_path: str = "/home/cx/AET_FOR_RL/vla/valid_original_data/droid_100/task_descriptions.json",
                 batch_size: int = 8,
                 action_chunk_size: int = 16,
                 max_sequence_length: int = 50):

        print(f"ğŸ”§ åˆå§‹åŒ–è½¯ä½“è‡‚OpenPiæ•°æ®é›†")
        print(f"   è½¯ä½“è‡‚æ•°æ®: {soft_arm_data_dir}")
        print(f"   DROIDå›¾åƒ: {droid_images_dir}")

        self.soft_arm_data_dir = Path(soft_arm_data_dir)
        self.droid_images_dir = Path(droid_images_dir)
        self.batch_size = batch_size
        self.action_chunk_size = action_chunk_size
        self.max_sequence_length = max_sequence_length

        # åŠ è½½DROIDå…ƒæ•°æ®
        self.droid_df = pd.read_parquet(droid_parquet_path)

        # åŠ è½½ä»»åŠ¡æè¿°
        if os.path.exists(task_descriptions_path):
            with open(task_descriptions_path, 'r') as f:
                task_data = json.load(f)
                self.task_descriptions = {
                    str(ep): desc for ep, desc in zip(
                        task_data.get('valid_episode_list', []),
                        task_data.get('task_descriptions', [])
                    )
                }
        else:
            print(f"âš ï¸ ä»»åŠ¡æè¿°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æè¿°")
            self.task_descriptions = {}

        # æ‰«æè½¯ä½“è‡‚é…ç½®
        self.samples = self._scan_soft_arm_data()
        print(f"âœ… æ•°æ®é›†å°±ç»ª: {len(self.samples)} ä¸ªæ ·æœ¬")

    def _scan_soft_arm_data(self) -> List[Dict]:
        """æ‰«ææ‰€æœ‰è½¯ä½“è‡‚é…ç½®æ•°æ®"""
        samples = []

        if not self.soft_arm_data_dir.exists():
            print(f"âŒ è½¯ä½“è‡‚æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.soft_arm_data_dir}")
            return samples

        # éå†æ‰€æœ‰episode
        for episode_dir in self.soft_arm_data_dir.glob("episode_*"):
            original_episode = int(episode_dir.name.split('_')[1])

            # éå†æ‰€æœ‰é…ç½®ï¼ˆ2,3,4,5æ®µï¼‰
            for config_dir in episode_dir.glob("*_segments"):
                segments = int(config_dir.name.split('_')[0])

                # æ£€æŸ¥å¿…è¦æ–‡ä»¶
                traj_file = config_dir / "joint_trajectory.npz"
                config_file = config_dir / "config.json"

                if traj_file.exists() and config_file.exists():
                    samples.append({
                        'original_episode': original_episode,
                        'segments': segments,
                        'trajectory_path': traj_file,
                        'config_path': config_file,
                        'episode_dir': episode_dir.name,
                        'config_dir': config_dir.name
                    })

        return samples

    def _load_soft_arm_trajectory(self, traj_path: Path) -> Dict:
        """åŠ è½½è½¯ä½“è‡‚è½¨è¿¹æ•°æ®"""
        data = np.load(traj_path)
        return {
            'joint_positions': data['joint_positions'],           # (T, N) å…³èŠ‚è§’åº¦
            'timestamps': data['timestamps'],                     # (T,) æ—¶é—´æˆ³
            'end_effector_positions': data['end_effector_positions'], # (T, 3) æœ«ç«¯ä½ç½®
            'success_mask': data['success_mask'],                 # (T,) æˆåŠŸæ©ç 
            'constraint_type': str(data.get('constraint_type', '3DOF'))
        }

    def _load_droid_images(self, original_episode: int, num_frames: int) -> torch.Tensor:
        """åŠ è½½å¯¹åº”episodeçš„DROIDå›¾åƒ"""
        episode_image_dir = self.droid_images_dir / f"episode_{original_episode:03d}"

        # ä¼˜å…ˆä½¿ç”¨exterior_image_1_leftï¼ˆå¤–éƒ¨è§†è§’ï¼‰
        camera_dirs = ['exterior_image_1_left', 'exterior_image_2_left', 'wrist_image_left']

        for camera_dir in camera_dirs:
            image_dir = episode_image_dir / camera_dir
            if image_dir.exists():
                break
        else:
            # åˆ›å»ºfallbackå›¾åƒ
            print(f"âš ï¸ Episode {original_episode} å›¾åƒä¸å­˜åœ¨ï¼Œä½¿ç”¨fallback")
            return self._create_fallback_images(num_frames)

        # åŠ è½½å›¾åƒåºåˆ—
        images = []
        image_files = sorted(image_dir.glob("frame_*.jpg"))

        for i in range(min(num_frames, len(image_files))):
            if i < len(image_files):
                img_path = image_files[i]
                img = Image.open(img_path).convert('RGB')
                img = img.resize((224, 224))  # OpenPiæ ‡å‡†å°ºå¯¸
                img_array = np.array(img) / 255.0
                images.append(img_array)
            else:
                # é‡å¤æœ€åä¸€å¸§
                images.append(images[-1] if images else np.zeros((224, 224, 3)))

        return torch.tensor(np.stack(images), dtype=torch.float32)  # (T, H, W, C)

    def _create_fallback_images(self, num_frames: int) -> torch.Tensor:
        """åˆ›å»ºfallbackå›¾åƒåºåˆ—"""
        # ç®€å•çš„ç»“æ„åŒ–å›¾åƒ
        fallback = np.zeros((num_frames, 224, 224, 3))

        for t in range(num_frames):
            img = np.zeros((224, 224, 3))
            # æ¡Œé¢ (æ£•è‰²)
            img[150:, :] = [0.4, 0.3, 0.2]
            # æœºæ¢°è‡‚ (ç°è‰²)
            img[80:160, 50:150] = [0.5, 0.5, 0.5]
            # ç‰©ä½“ (çº¢è‰²ï¼Œéšæ—¶é—´ç§»åŠ¨)
            obj_x = int(100 + 20 * np.sin(t * 0.1))
            img[120:140, obj_x:obj_x+20] = [0.8, 0.2, 0.2]

            fallback[t] = img

        return torch.tensor(fallback, dtype=torch.float32)

    def _get_task_description(self, original_episode: int) -> str:
        """è·å–ä»»åŠ¡æè¿°"""
        episode_key = str(original_episode)
        if episode_key in self.task_descriptions:
            return self.task_descriptions[episode_key]
        else:
            return "Complete the manipulation task with the soft continuum arm"

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict:
        """è·å–å•ä¸ªè®­ç»ƒæ ·æœ¬ï¼ŒOpenPiæ ¼å¼"""
        sample = self.samples[idx]

        # åŠ è½½è½¯ä½“è‡‚è½¨è¿¹
        trajectory = self._load_soft_arm_trajectory(sample['trajectory_path'])
        joint_positions = trajectory['joint_positions']  # (T, N)

        # é™åˆ¶åºåˆ—é•¿åº¦
        seq_length = min(len(joint_positions), self.max_sequence_length)
        joint_positions = joint_positions[:seq_length]  # (seq_len, N)

        # åŠ è½½é…ç½®ä¿¡æ¯
        with open(sample['config_path'], 'r') as f:
            config = json.load(f)

        # åŠ è½½å¯¹åº”çš„DROIDå›¾åƒ
        images = self._load_droid_images(sample['original_episode'], seq_length)  # (seq_len, H, W, C)

        # è·å–ä»»åŠ¡æè¿°
        task_description = self._get_task_description(sample['original_episode'])

        # è½¬æ¢ä¸ºOpenPiæœŸæœ›çš„æ ¼å¼
        return {
            # è§†è§‰è¾“å…¥
            'image': images,  # (seq_len, H, W, C) - OpenPiæœŸæœ›æ ¼å¼

            # è¯­è¨€è¾“å…¥
            'language_instruction': task_description,

            # åŠ¨ä½œè¾“å‡º - é‡å¡‘ä¸ºaction chunks
            'action': self._format_actions_for_openpi(joint_positions),  # (seq_len, action_dim)

            # å…ƒæ•°æ®
            'episode_id': sample['original_episode'],
            'segments': sample['segments'],
            'constraint_type': trajectory['constraint_type'],
            'robot_config': {
                'n_segments': config['n_segments'],
                'segment_lengths': config['segment_lengths'],
                'base_offset': config['base_offset'],
            }
        }

    def _format_actions_for_openpi(self, joint_positions: np.ndarray) -> torch.Tensor:
        """å°†è½¯ä½“è‡‚å…³èŠ‚è§’åº¦æ ¼å¼åŒ–ä¸ºOpenPiåŠ¨ä½œæ ¼å¼"""
        # joint_positions: (seq_len, n_joints)
        seq_len, n_joints = joint_positions.shape

        # OpenPiæœŸæœ›çš„åŠ¨ä½œchunkæ ¼å¼
        # è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°å°†å…³èŠ‚è§’åº¦ä½œä¸ºç›®æ ‡åŠ¨ä½œ
        actions = torch.tensor(joint_positions, dtype=torch.float32)

        # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ gripperåŠ¨ä½œç»´åº¦
        # actions = torch.cat([actions, torch.zeros(seq_len, 1)], dim=-1)  # æ·»åŠ gripper

        return actions

def create_openpi_dataloader(batch_size: int = 8) -> torch.utils.data.DataLoader:
    """åˆ›å»ºOpenPiå…¼å®¹çš„æ•°æ®åŠ è½½å™¨"""
    dataset = SoftArmOpenPiDataset(batch_size=batch_size)

    def collate_fn(batch):
        """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå¤„ç†å˜é•¿åºåˆ—"""
        # æŒ‰segmentsåˆ†ç»„ï¼Œé¿å…ä¸åŒDOFçš„å¼ é‡æ··åˆ
        segments_to_batch = {}
        for item in batch:
            segments = item['segments']
            if segments not in segments_to_batch:
                segments_to_batch[segments] = []
            segments_to_batch[segments].append(item)

        # ä½¿ç”¨æœ€å¤§çš„ç»„
        largest_group = max(segments_to_batch.keys(), key=lambda k: len(segments_to_batch[k]))
        selected_batch = segments_to_batch[largest_group]

        # æ ‡å‡†collate
        collated = {}
        for key in selected_batch[0].keys():
            if key in ['image', 'action']:
                # å †å å¼ é‡æ•°æ®
                collated[key] = torch.stack([item[key] for item in selected_batch])
            elif key in ['episode_id', 'segments']:
                # æ•°å€¼æ•°æ®
                collated[key] = torch.tensor([item[key] for item in selected_batch])
            else:
                # åˆ—è¡¨æ•°æ®
                collated[key] = [item[key] for item in selected_batch]

        return collated

    return torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_fn,
        pin_memory=True
    )

def test_dataset():
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•è½¯ä½“è‡‚OpenPiæ•°æ®é›†")

    dataset = SoftArmOpenPiDataset()
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©º")
        return

    # æµ‹è¯•å•ä¸ªæ ·æœ¬
    sample = dataset[0]
    print(f"âœ… æ ·æœ¬æ ¼å¼:")
    for key, value in sample.items():
        if isinstance(value, torch.Tensor):
            print(f"   {key}: {value.shape} {value.dtype}")
        elif isinstance(value, dict):
            print(f"   {key}: dict with {len(value)} keys")
        else:
            print(f"   {key}: {type(value)} = {str(value)[:50]}...")

    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    dataloader = create_openpi_dataloader(batch_size=2)
    batch = next(iter(dataloader))
    print(f"âœ… Batchæ ¼å¼:")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"   {key}: {value.shape} {value.dtype}")
        else:
            print(f"   {key}: {type(value)} length={len(value) if hasattr(value, '__len__') else 'N/A'}")

if __name__ == "__main__":
    test_dataset()