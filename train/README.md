# VLA Training System

Vision-Language-Action model with morphology awareness and multi-joint cooperation.

## ğŸ¯ Overview

A complete VLA training pipeline that combines:
- **Real DROID images** (external camera, 320Ã—180 RGB)
- **Morphology-aware descriptions** (text descriptions of robot variations)
- **Graph Neural Networks** (multi-joint cooperation)
- **LoRA adaptation** (parameter-efficient fine-tuning)

## ğŸ“ Project Structure

```
/home/cx/AET_FOR_RL/vla/train/
â”œâ”€â”€ vla_trainer.py              # Main training script
â”œâ”€â”€ vla_model.py                # RynnVLA + LoRA + GNN architecture
â”œâ”€â”€ vla_model_trained.pth       # Trained model (3.14GB)
â””â”€â”€ data/                       # Training data (6.0M)
    â”œâ”€â”€ droid_unified_morphology.json     # 13 augmented episodes
    â””â”€â”€ extracted_droid_images/           # 60 real DROID images
        â”œâ”€â”€ episode_0/ (30 images)
        â””â”€â”€ episode_2/ (30 images)
```

## ğŸš€ Training Results

```
âœ… Training completed successfully:
â”œâ”€â”€ Final loss: 0.117099
â”œâ”€â”€ Epochs: 10
â”œâ”€â”€ GPU memory: 3.59GB (stable)
â”œâ”€â”€ Total samples: 390
â””â”€â”€ Morphology variations: 8
```

## ğŸ”§ Architecture

**Model**: RynnVLA-7B base + LoRA (rank 32) + GNN
**Parameters**: 440.64M total (LoRA: 0.26M, GNN: 7.35M)
**Training strategy**: Frozen backbone + trainable adaptation layers

## ğŸ’¡ Key Innovation

**External Camera Insight**: Fixed external cameras provide same images for all morphology variations, eliminating need for image transformation.

## ğŸ§ª Usage

```python
# Train new model
python vla_trainer.py

# Load trained model
import torch
from vla_model import RealRynnVLALoRAGNN

model = RealRynnVLALoRAGNN()
checkpoint = torch.load('vla_model_trained.pth')
model.load_state_dict(checkpoint['model_state_dict'])
```

## ğŸ“Š Data

- **Real images**: 60 DROID frames extracted from episodes 0 and 2
- **Morphology data**: 13 augmented variations from 3 original episodes
- **Training samples**: 390 total (4.3x augmentation multiplier)

Built with [RynnVLA-001](https://github.com/GenEmbedded/RynnVLA) and DROID-100 dataset.