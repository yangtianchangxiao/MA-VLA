#!/usr/bin/env python3
"""
Complete VLA Trainer: Real DROID Images + Morphology Descriptions + IK Trajectories
Final integration of all components for true Vision-Language-Action training
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import json
import os
from pathlib import Path
from PIL import Image
import random
from typing import Dict, List, Tuple

# Import our real model
from vla_model import RealRynnVLALoRAGNN

class CompleteVLADataset(Dataset):
    """Complete VLA dataset with real DROID images, morphology descriptions, and IK trajectories"""
    
    def __init__(self, 
                 extracted_images_path="data/extracted_droid_images",
                 morphology_data_path="../training_data/merged_training_stats.json"):
        print(f"ğŸ“‚ Loading Complete VLA Dataset")
        print(f"   Images: {extracted_images_path}")
        print(f"   Morphology data: {morphology_data_path}")
        
        # Load extracted image data
        with open(Path(extracted_images_path) / "extraction_summary.json", 'r') as f:
            self.image_data = json.load(f)
        
        # Load DROID episode metadata for real language instructions
        import pandas as pd
        import numpy as np
        episodes_df = pd.read_parquet('/home/cx/AET_FOR_RL/vla/original_data/droid_100/meta/episodes/chunk-000/file-000.parquet')
        self.episode_tasks = {}
        for _, row in episodes_df.iterrows():
            episode_idx = str(row['episode_index'])
            if 'tasks' in row:
                tasks = row['tasks']
                if isinstance(tasks, np.ndarray) and len(tasks) > 0:
                    self.episode_tasks[episode_idx] = str(tasks[0])  # numpyæ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ª
                elif isinstance(tasks, list) and len(tasks) > 0:
                    self.episode_tasks[episode_idx] = str(tasks[0])
                elif isinstance(tasks, str) and tasks.strip():
                    self.episode_tasks[episode_idx] = tasks
        
        # Load morphology augmentation data
        with open(morphology_data_path, 'r') as f:
            self.morphology_data = json.load(f)
        
        # Create complete VLA samples
        self.samples = self._create_complete_samples()
        
        print(f"ğŸ“Š Complete VLA Dataset ready: {len(self.samples)} samples")
        
        # Show sample variations
        variations = set(s['variation'] for s in self.samples)
        print(f"ğŸ¤– Morphology variations: {sorted(variations)}")
        print(f"ğŸ‘ï¸  Image episodes: {sorted(self.image_data['episodes'].keys())}")
    
    def _create_complete_samples(self):
        """Create complete VLA samples combining images, text, and trajectories"""
        samples = []
        
        print(f"ğŸ” Processing {len(self.morphology_data['episodes'])} morphology episodes...")
        for i, morph_episode in enumerate(self.morphology_data['episodes']):
            if i < 5:  # Debug first 5
                print(f"   Episode {i}: original_episode={morph_episode['original_episode']}, variation={morph_episode['variation_type']}")
            
            original_episode = str(morph_episode['original_episode'])
            variation = morph_episode['variation_type']
            
            # Check if we have images for this original episode  
            if original_episode not in self.image_data['episodes']:
                if i < 5:  # Only show first few warnings
                    print(f"âš ï¸  Episode {original_episode} not found in image data, skipping")
                continue
            
            # Get task-generic description (VLAåŸåˆ™)
            # VLAåŸåˆ™ï¼šè¯­è¨€æŒ‡ä»¤å¿…é¡»æ˜¯ä»»åŠ¡é€šç”¨çš„ï¼Œä¸åŒ…å«å½¢æ€å­¦ä¿¡æ¯
            # ä»DROIDæ•°æ®é›†è·å–çœŸå®çš„ä»»åŠ¡æŒ‡ä»¤
            original_episode_str = str(original_episode)
            if original_episode_str in self.episode_tasks:
                description = self.episode_tasks[original_episode_str]
            else:
                description = "Complete the manipulation task"  # ç®€å•fallback
            
            # Get trajectory data  
            import numpy as np
            actions_str = morph_episode['actions']
            try:
                # å¤„ç†numpyæ•°ç»„å­—ç¬¦ä¸²æ ¼å¼ï¼Œå…ˆè§£æå†æ ¹æ®å®é™…ç»´åº¦è°ƒæ•´
                actions_cleaned = actions_str.replace('[', '').replace(']', '').replace('\n', ' ')
                actions_flat = np.fromstring(actions_cleaned, sep=' ')
                
                # æ ¹æ®å®é™…æ•°æ®æ¨æ–­åˆ—æ•°
                if len(actions_flat) % 5 == 0:
                    num_cols = 5  # 5-DOF
                elif len(actions_flat) % 6 == 0:
                    num_cols = 6  # å¯èƒ½æ˜¯5-DOF + gripper  
                elif len(actions_flat) % 7 == 0:
                    num_cols = 7  # 7-DOF
                else:
                    if i < 5:
                        print(f"âŒ Cannot determine joint count for {len(actions_flat)} values")
                    continue
                    
                actions = actions_flat.reshape(-1, num_cols)
                
                # å¦‚æœæ˜¯6åˆ—ä½†æ˜¯DOF modificationï¼Œå–å‰5åˆ—ï¼ˆå»é™¤gripperï¼‰
                if num_cols == 6 and 'dof_modification' in variation:
                    actions = actions[:, :5]
                if i < 2:  # Debug first 2
                    print(f"   Actions parsed: {actions.shape}")
            except Exception as e:
                if i < 5:
                    print(f"âŒ Actions parsing failed for episode {i}: {e}, trying alternative...")
                    print(f"   First 200 chars: {actions_str[:200]}")
                continue
            
            # Get image data for original episode
            image_episode_data = self.image_data['episodes'][original_episode]
            image_frames = image_episode_data['frame_data']
            
            # Match trajectory timesteps with image frames
            traj_length = len(actions)
            img_length = len(image_frames)
            
            # Sample frames to match trajectory length
            if img_length >= traj_length:
                # Use first traj_length images
                selected_frames = image_frames[:traj_length]
            else:
                # Repeat last image if needed
                selected_frames = image_frames + [image_frames[-1]] * (traj_length - img_length)
            
            # Create individual samples
            for t in range(min(traj_length, 30)):  # Limit to 30 timesteps for training efficiency
                if t < len(selected_frames):
                    sample = {
                        'episode_id': morph_episode['episode_id'],
                        'timestep': t,
                        'variation': variation,
                        'task_instruction': description,
                        'morphology_config': morph_episode.get('morphology_config', 'Unknown morphology'),
                        'joint_state': actions[t],  # åœ¨VLAä¸­ï¼ŒåŠ¨ä½œå³çŠ¶æ€
                        'joint_action': actions[t],
                        'image_path': f"data/extracted_droid_images/episode_{original_episode}/frame_{t:04d}_exterior_image_1_left.png",
                        'original_episode': int(original_episode),
                        'task_context': f"Episode {morph_episode['episode_id']}: {description}"
                    }
                    samples.append(sample)
        
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def _load_image(self, image_path):
        """Load real DROID image"""
        try:
            if os.path.exists(image_path):
                img = Image.open(image_path).convert('RGB')
                img_array = np.array(img) / 255.0  # Normalize to [0, 1]
                return torch.tensor(img_array, dtype=torch.float32).permute(2, 0, 1)  # (3, H, W)
            else:
                print(f"âš ï¸  Image not found: {image_path}")
                return self._create_fallback_image()
        except Exception as e:
            print(f"âš ï¸  Error loading image {image_path}: {e}")
            return self._create_fallback_image()
    
    def _create_fallback_image(self):
        """Create fallback image if real image loading fails"""
        # Simple structured synthetic image as fallback
        image = torch.zeros(3, 180, 320)
        # Table (brown)
        image[0, 120:, :] = 0.4
        image[1, 120:, :] = 0.3
        image[2, 120:, :] = 0.2
        # Robot arm (gray)
        image[:, 60:140, 100:220] = 0.5
        # Object (red)
        image[0, 100:120, 160:180] = 0.8
        image[1, 100:120, 160:180] = 0.2
        image[2, 100:120, 160:180] = 0.2
        return image
    
    def _tokenize_description(self, description):
        """Simple tokenization for morphology-aware descriptions"""
        # Convert to lowercase and limit length
        text = description.lower()[:64]
        # Convert to ASCII codes, limit to reasonable vocab range
        tokens = [min(ord(c), 999) for c in text]
        
        # Pad to fixed length
        max_len = 32
        if len(tokens) < max_len:
            tokens.extend([0] * (max_len - len(tokens)))
        else:
            tokens = tokens[:max_len]
        
        return torch.tensor(tokens, dtype=torch.long)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. Load real DROID image
        image = self._load_image(sample['image_path'])  # (3, 180, 320)
        
        # 2. Tokenize task-generic description (VLAåŸåˆ™)  
        description_tokens = self._tokenize_description(sample['task_instruction'])  # (32,)
        
        # 3. Current joint state
        joint_state = torch.tensor(sample['joint_state'], dtype=torch.float32)  # (7,)
        
        # 4. Morphology configuration
        morphology = sample['morphology_config']
        # morphology_configå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
        if isinstance(morphology, str):
            # ç®€å•fallbackï¼Œæå–link_scalesä¿¡æ¯
            if 'link_scales' in morphology:
                import re
                scales_match = re.search(r'link_scales=\[([\d\.,\s]+)\]', morphology)
                if scales_match:
                    scales_str = scales_match.group(1)
                    scaled_links = [float(x.strip()) for x in scales_str.split(',')]
                else:
                    scaled_links = [1.0] * 7  # Default
            else:
                scaled_links = [1.0] * 7  # Default for DOF modifications
        else:
            scaled_links = morphology.get('scaled_link_lengths', [0.333, 0.316, 0.384, 0.088, 0.107, 0.103, 0.0])
        # æå–base positionå’Œorientationï¼ˆå¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        if isinstance(morphology, str):
            base_pos = [0, 0, 0]
            base_ori = [0, 0, 0]  
        else:
            base_pos = morphology.get('base_position', [0, 0, 0])
            base_ori = morphology.get('base_orientation', [0, 0, 0])
        
        morphology_features = torch.tensor([
            scaled_links[0],  # Link 1 scale
            scaled_links[1],  # Link 2 scale  
            scaled_links[2],  # Link 3 scale
            base_pos[0],      # Base X position
            base_pos[1],      # Base Y position
            base_ori[2],      # Base Z rotation
        ], dtype=torch.float32)  # (6,)
        
        # 5. Target action (IK-retargeted) - convert to action sequence
        action_data = sample['joint_action']
        action_chunk_size = 20  # Match RynnVLA sequence length

        if len(action_data) == 5:  # 5-DOF robot
            single_action = torch.tensor(action_data, dtype=torch.float32)  # (5,)
            # Repeat single action to create sequence (simple approach)
            # Shape: (num_joints, action_chunk_size) to match model output
            target_action = single_action.unsqueeze(1).repeat(1, action_chunk_size)  # (5, 20)
        elif len(action_data) == 6:  # 5-DOF + gripper
            single_action = torch.tensor(action_data, dtype=torch.float32)  # (6,)
            target_action = single_action.unsqueeze(1).repeat(1, action_chunk_size)  # (6, 20)
        elif len(action_data) == 7:  # 7-DOF robot
            single_action = torch.tensor(action_data, dtype=torch.float32)  # (7,)
            target_action = single_action.unsqueeze(1).repeat(1, action_chunk_size)  # (7, 20)
        else:
            raise ValueError(f"Unsupported action dimension: {len(action_data)}")

        # Store DOF info for GNN processing
        sample_dof = len(action_data)
        
        return {
            'image': image,                                    # (3, H, W) - Real DROID image
            'description_tokens': description_tokens,          # (32,) - Task-generic description
            'joint_state': joint_state,                       # (7,) - Current joint angles
            'morphology_features': morphology_features,       # (6,) - Morphology configuration
            'target_action': target_action,                   # Variable DOF - IK-retargeted action
            'dof': sample_dof,                               # Number of DOF for this sample
            'episode_id': sample['episode_id'],              # Metadata
            'variation': sample['variation'],                 # Morphology variation
            'description': sample['task_instruction']   # Task-generic description
        }

def morphology_collate_fn(batch):
    """Custom collate function for variable DOF morphology data"""
    # Separate by DOF type to avoid tensor size mismatch
    dof_5_batch = []
    dof_6_batch = []
    dof_7_batch = []
    
    for sample in batch:
        if sample['dof'] == 5:
            dof_5_batch.append(sample)
        elif sample['dof'] == 6:
            dof_6_batch.append(sample)
        elif sample['dof'] == 7:
            dof_7_batch.append(sample)
    
    # Process the largest group first (most efficient)
    batches_to_process = []
    if dof_7_batch: batches_to_process.append(('7dof', dof_7_batch))
    if dof_6_batch: batches_to_process.append(('6dof', dof_6_batch))
    if dof_5_batch: batches_to_process.append(('5dof', dof_5_batch))
    
    # For now, just use the first group to avoid mixing DOF in one batch
    # TODO: Improve GNN to handle mixed DOF in one batch
    if not batches_to_process:
        return None
    
    # Use the first (largest) group of same-DOF samples
    dof_type, selected_batch = batches_to_process[0]
    
    # Standard collating for same-DOF samples
    collated = {}
    for key in selected_batch[0].keys():
        if key in ['target_action']:
            # Stack actions of same DOF
            collated[key] = torch.stack([sample[key] for sample in selected_batch])
        elif key in ['image', 'description_tokens', 'joint_state', 'morphology_features']:
            # Stack tensor data
            collated[key] = torch.stack([sample[key] for sample in selected_batch])
        elif key == 'dof':
            # All samples have same DOF in this batch
            collated[key] = selected_batch[0][key]
        else:
            # List data (strings, etc.)
            collated[key] = [sample[key] for sample in selected_batch]
    
    return collated

def train_complete_vla():
    """Train complete VLA with real images, morphology text, and IK trajectories"""
    print("ğŸš€ Training Complete VLA: Real Images + Morphology Text + IK Trajectories")
    print("=" * 100)
    
    # Setup device - use cuda:3 as requested
    if torch.cuda.is_available() and torch.cuda.device_count() > 3:
        device = torch.device('cuda:3')
    elif torch.cuda.is_available():
        device = torch.device('cuda:0')
    else:
        device = torch.device('cpu')
    
    print(f"ğŸ”§ Using device: {device}")
    
    # Memory monitoring function
    def get_gpu_memory():
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated(device) / 1024**3  # GB
        return 0
    
    # Check initial GPU memory
    initial_memory = get_gpu_memory()
    print(f"ğŸ” Initial GPU memory: {initial_memory:.2f} GB")
    
    # Load complete VLA dataset
    dataset = CompleteVLADataset()
    dataloader = DataLoader(dataset, batch_size=6, shuffle=True, num_workers=0, collate_fn=morphology_collate_fn)
    print(f"ğŸ“Š Dataset: {len(dataset)} samples, {len(dataloader)} batches")
    
    # Check memory after dataset loading
    dataset_memory = get_gpu_memory()
    print(f"ğŸ” Memory after dataset: {dataset_memory:.2f} GB (+{dataset_memory-initial_memory:.2f} GB)")
    
    # Create model
    model_path = "/home/cx/AET_FOR_RL/vla/å‚è€ƒæ¨¡å‹/RynnVLA-001/pretrained_models/RynnVLA-001-7B-Base"
    model = RealRynnVLALoRAGNN(
        model_path=model_path,
        lora_rank=8,       # Smaller LoRA to reduce interference with original weights
        gnn_node_dim=256,  # Rich GNN representations
        action_chunk_size=20  # Output action sequences like RynnVLA
    ).to(device)
    
    # Create complete VLA wrapper
    class CompleteVLAWrapper(nn.Module):
        """Complete VLA model with real vision + language + morphology + action"""
        
        def __init__(self, base_model):
            super().__init__()
            self.base_model = base_model
            hidden_size = base_model.hidden_size
            
            # Vision encoder for real DROID images (320x180)
            self.vision_encoder = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),      # -> 160x90
                nn.ReLU(),
                nn.BatchNorm2d(64),
                nn.MaxPool2d(2),                                           # -> 80x45
                nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # -> 40x23  
                nn.ReLU(),
                nn.BatchNorm2d(128),
                nn.AdaptiveAvgPool2d((8, 8)),                              # -> 8x8
                nn.Flatten(),
                nn.Linear(128 * 8 * 8, 2048),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(2048, hidden_size)
            )
            
            # Language encoder for morphology descriptions
            self.language_encoder = nn.Sequential(
                nn.Linear(32, 512),   # Token sequence to embedding
                nn.ReLU(),
                nn.Linear(512, hidden_size)
            )
            
            # Morphology encoder - use LayerNorm instead of BatchNorm for batch_size=1 compatibility
            self.morphology_encoder = nn.Sequential(
                nn.Linear(6, 256),
                nn.ReLU(),
                nn.LayerNorm(256),
                nn.Linear(256, 512),
                nn.ReLU(),
                nn.Linear(512, hidden_size)
            )
            
            # Multimodal fusion
            self.fusion_attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)
            self.fusion_norm = nn.LayerNorm(hidden_size)
            self.fusion_mlp = nn.Sequential(
                nn.Linear(hidden_size, hidden_size * 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_size * 2, hidden_size)
            )
        
        def forward(self, images, description_tokens, morphology_features, num_joints=7):
            batch_size = images.shape[0]
            
            # 1. Encode modalities
            vision_features = self.vision_encoder(images)          # (B, hidden_size)
            language_features = self.language_encoder(description_tokens.float())  # (B, hidden_size)
            morphology_enc = self.morphology_encoder(morphology_features)  # (B, hidden_size)
            
            # 2. Multimodal attention fusion
            # Stack modalities for attention
            multimodal_input = torch.stack([vision_features, language_features, morphology_enc], dim=1)  # (B, 3, hidden_size)
            
            # Self-attention across modalities
            attended, _ = self.fusion_attention(multimodal_input, multimodal_input, multimodal_input)
            
            # Residual connection and norm
            fused = self.fusion_norm(attended + multimodal_input)
            
            # MLP and final pooling
            enhanced = self.fusion_mlp(fused)
            final_features = enhanced.mean(dim=1)  # Average across modalities (B, hidden_size)
            
            # 3. Process through base model GNN with variable DOF
            return self._process_through_gnn(final_features, num_joints)
        
        def _process_through_gnn(self, fused_features, num_joints=7):
            """Process through LoRA + GNN architecture with variable DOF"""
            batch_size = fused_features.shape[0]
            
            # Apply LoRA adaptation
            adapted = fused_features + self.base_model.lora_projection(fused_features.unsqueeze(1)).squeeze(1)
            
            # Final norm
            normalized = self.base_model.final_norm(adapted.unsqueeze(1)).squeeze(1)
            
            # Convert to joint nodes with variable DOF
            joint_nodes = self.base_model.to_joint_nodes(normalized, num_joints=num_joints)  # [batch, num_joints, node_dim]
            
            # GNN processing with variable topology
            updated_nodes = self.base_model.robot_graph(joint_nodes)
            
            # Action decoding - naturally handles variable DOF
            actions = self.base_model.graph_decoder(updated_nodes)
            
            return {
                'actions': actions,
                'hidden_state': normalized,
                'node_features': updated_nodes
            }
    
    # Wrap model
    complete_vla = CompleteVLAWrapper(model).to(device)
    
    # Check memory after model loading
    model_memory = get_gpu_memory()
    print(f"ğŸ” Memory after model: {model_memory:.2f} GB (+{model_memory-initial_memory:.2f} GB)")
    
    # Training setup
    trainable_params = [p for p in complete_vla.parameters() if p.requires_grad]
    optimizer = optim.AdamW(trainable_params, lr=3e-5, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)
    # Use L1Loss following OpenVLA-OFT SOTA practice
    criterion = nn.L1Loss()  # Better for robotics actions than MSE
    
    print(f"ğŸ¯ Trainable parameters: {sum(p.numel() for p in trainable_params)/1e6:.2f}M")
    
    # Training loop
    num_epochs = 3  # Reduce epochs to avoid overfitting with L1 loss
    best_loss = float('inf')
    
    for epoch in range(num_epochs):
        complete_vla.train()
        epoch_loss = 0.0
        num_batches = 0
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs}")
        
        for batch_idx, batch in enumerate(dataloader):
            optimizer.zero_grad()
            
            # Move to device
            images = batch['image'].to(device)                          # Real DROID images
            description_tokens = batch['description_tokens'].to(device) # Task-generic descriptions
            morphology_features = batch['morphology_features'].to(device) # Morphology config
            target_actions = batch['target_action'].to(device)         # IK-retargeted actions
            num_joints = batch['dof']                                   # DOF for this batch
            
            # Forward pass with variable DOF
            outputs = complete_vla(images, description_tokens, morphology_features, num_joints=num_joints)
            predicted_actions = outputs['actions']
            
            # Loss
            loss = criterion(predicted_actions, target_actions)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 5 == 0:
                current_memory = get_gpu_memory()
                print(f"   Batch {batch_idx:3d}: Loss = {loss.item():.6f}, Memory = {current_memory:.2f} GB")
                # Show sample
                if batch_idx == 0:
                    sample_pred = predicted_actions[0].cpu()  # [num_joints, 20]
                    sample_target = target_actions[0].cpu()  # [num_joints, 20]
                    sample_desc = batch['description'][0]
                    sample_var = batch['variation'][0]
                    print(f"      Description: '{sample_desc[:50]}...'")
                    print(f"      Variation: {sample_var}")
                    # Show first timestep of first few joints
                    print(f"      Target (t=0):  [{sample_target[0,0]:.3f}, {sample_target[1,0]:.3f}, {sample_target[2,0]:.3f}]...")
                    print(f"      Predict (t=0): [{sample_pred[0,0]:.3f}, {sample_pred[1,0]:.3f}, {sample_pred[2,0]:.3f}]...")
        
        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0
        scheduler.step()
        
        print(f"ğŸ“Š Epoch {epoch+1}: Avg Loss = {avg_loss:.6f}, LR = {optimizer.param_groups[0]['lr']:.2e}")
        
        # Save best model
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save({
                'model_state_dict': complete_vla.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
                'epoch': epoch,
                'components': {
                    'vision': 'Real DROID images (320x180 RGB)',
                    'language': 'Morphology-aware descriptions',
                    'action': 'IK-retargeted trajectories',
                    'architecture': 'RynnVLA + LoRA + GNN + Multimodal Fusion'
                }
            }, 'vla_model_trained.pth')
            print(f"   ğŸ’¾ Saved best model (loss: {best_loss:.6f})")
    
    print(f"\nğŸ‰ Complete VLA Training Finished!")
    print(f"   ğŸ† Best loss: {best_loss:.6f}")
    print(f"   ğŸ“Š Components integrated:")
    print(f"     ğŸ‘ï¸  Vision: Real DROID external camera images")
    print(f"     ğŸ—£ï¸  Language: Morphology-aware task descriptions")
    print(f"     ğŸ¤– Action: IK-retargeted multi-morphology trajectories")
    print(f"     ğŸ§  Architecture: RynnVLA backbone + LoRA adaptation + GNN cooperation")
    
    return complete_vla

if __name__ == "__main__":
    try:
        vla_model = train_complete_vla()
        print(f"\nâœ… SUCCESS! Complete VLA model ready!")
        print(f"ğŸ¯ This is now a TRUE Vision-Language-Action model!")
        print(f"ğŸ’¾ Model saved and training completed successfully!")
        print(f"ğŸ“Š Press Ctrl+C to exit or inspect the session...")
        
        # Keep session alive so user can inspect results
        import time
        while True:
            print(f"ğŸ”„ Training completed. Session kept alive for inspection...")
            time.sleep(60)  # Print status every minute
        
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ Session terminated by user")
    except Exception as e:
        import traceback
        print(f"âŒ Training error: {e}")
        traceback.print_exc()
        print(f"ğŸ’€ Session kept alive for debugging. Press Ctrl+C to exit...")
        
        # Keep session alive even on error for debugging
        import time
        while True:
            print(f"ğŸ” Error occurred. Session kept alive for debugging...")
            time.sleep(60)