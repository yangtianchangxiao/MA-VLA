#!/usr/bin/env python3
"""
OpenPiè½¯ä½“è‡‚8å¡è®­ç»ƒè„šæœ¬
åŸºäºÏ€â‚€.â‚… DROIDå¾®è°ƒç‰ˆæœ¬ï¼Œé€‚é…è½¯ä½“è‡‚æ•°æ®

ä½¿ç”¨æ–¹æ³•:
    å•GPU: python train_soft_arm_openpi_8gpu.py
    8GPU:  torchrun --nproc_per_node=8 train_soft_arm_openpi_8gpu.py
"""

import os
import sys
import argparse
import json
from datetime import datetime
from typing import Dict, Any, Optional

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import numpy as np
from tqdm import tqdm

# æ·»åŠ OpenPiè·¯å¾„
sys.path.append('/home/cx/AET_FOR_RL/vla/å‚è€ƒæ¨¡å‹/openpi')
sys.path.append('/home/cx/AET_FOR_RL/vla')

from openpi_soft_arm_dataloader import create_soft_arm_dataloaders
from openpi.training import config as openpi_config

def init_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])

        print(f"ğŸš€ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ: rank={rank}, world_size={world_size}, local_rank={local_rank}")

        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(local_rank)

        return rank, world_size, local_rank
    else:
        print("ğŸ”§ å•GPUè®­ç»ƒæ¨¡å¼")
        return 0, 1, 0

def get_soft_arm_config() -> Dict[str, Any]:
    """è·å–è½¯ä½“è‡‚è®­ç»ƒé…ç½®"""
    return {
        # æ¨¡å‹é…ç½®
        'model_name': 'pi05_droid',  # ä½¿ç”¨DROIDå¾®è°ƒç‰ˆæœ¬
        'pretrained_checkpoint': '~/.cache/openpi/checkpoints/pi05_droid',

        # æ•°æ®é…ç½®
        'batch_size': 8,  # æ¯GPUæ‰¹é‡å¤§å°
        'num_workers': 4,
        'train_split': 0.9,
        'image_size': (224, 224),
        'action_chunk_size': 16,
        'max_sequence_length': 50,

        # è®­ç»ƒé…ç½®
        'num_epochs': 20,
        'learning_rate': 1e-4,  # æ¯”é¢„è®­ç»ƒæ›´å°çš„å­¦ä¹ ç‡
        'weight_decay': 1e-5,
        'warmup_steps': 500,
        'save_interval': 1000,
        'eval_interval': 500,
        'log_interval': 50,

        # è½¯ä½“è‡‚ä¸“ç”¨
        'max_action_dim': 12,  # æ”¯æŒæœ€å¤š6æ®µÃ—2å‚æ•°
        'constraint_types': ['3DOF', '4DOF'],

        # ä¼˜åŒ–å™¨
        'optimizer': 'adamw',
        'beta1': 0.9,
        'beta2': 0.95,
        'grad_clip_norm': 1.0,

        # è¾“å‡ºç›®å½•
        'output_dir': '/home/cx/AET_FOR_RL/vla/checkpoints/soft_arm_openpi',
        'experiment_name': f"soft_arm_pi05_droid_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    }

class SoftArmVLAModel(nn.Module):
    """è½¯ä½“è‡‚VLAæ¨¡å‹é€‚é…å™¨"""

    def __init__(self, base_model, max_action_dim: int = 12):
        super().__init__()
        self.base_model = base_model
        self.max_action_dim = max_action_dim

        # è·å–åŸå§‹åŠ¨ä½œå¤´çš„è¾“å…¥ç»´åº¦
        if hasattr(base_model, 'action_head'):
            original_dim = base_model.action_head.in_features
            self.action_head = nn.Linear(original_dim, max_action_dim)
        else:
            # å¦‚æœæ‰¾ä¸åˆ°åŠ¨ä½œå¤´ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„é€‚é…å±‚
            self.action_head = nn.Linear(768, max_action_dim)  # å‡è®¾768ç»´ç‰¹å¾

        print(f"âœ… è½¯ä½“è‡‚åŠ¨ä½œå¤´: {self.action_head}")

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹æå–ç‰¹å¾
        features = self.base_model.encode(
            images=batch['image'],
            instructions=batch['instruction']
        )

        # ç”Ÿæˆè½¯ä½“è‡‚åŠ¨ä½œ
        actions = self.action_head(features)

        return {
            'actions': actions,
            'features': features,
        }

def load_openpi_model(config: Dict[str, Any], device: torch.device):
    """åŠ è½½OpenPiæ¨¡å‹"""
    try:
        # è·å–DROIDé…ç½®
        openpi_cfg = openpi_config.get_config('pi05_droid')
        print(f"âœ… åŠ è½½OpenPié…ç½®: pi05_droid")

        # åˆ›å»ºåŸºç¡€æ¨¡å‹
        from openpi.models import create_model
        base_model = create_model(openpi_cfg)

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        checkpoint_path = os.path.expanduser(config['pretrained_checkpoint'])
        if os.path.exists(checkpoint_path):
            print(f"ğŸ”½ åŠ è½½é¢„è®­ç»ƒæƒé‡: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            base_model.load_state_dict(checkpoint['model'], strict=False)
            print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
        else:
            print(f"âš ï¸ é¢„è®­ç»ƒæƒé‡ä¸å­˜åœ¨: {checkpoint_path}")
            print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")

        # åˆ›å»ºè½¯ä½“è‡‚é€‚é…æ¨¡å‹
        model = SoftArmVLAModel(base_model, config['max_action_dim'])
        model = model.to(device)

        return model

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def train_epoch(model: nn.Module,
                train_loader,
                optimizer: torch.optim.Optimizer,
                criterion: nn.Module,
                device: torch.device,
                epoch: int,
                config: Dict[str, Any],
                rank: int = 0) -> Dict[str, float]:
    """è®­ç»ƒä¸€ä¸ªepoch"""

    model.train()
    total_loss = 0.0
    total_samples = 0

    if rank == 0:
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    else:
        pbar = train_loader

    for step, batch in enumerate(pbar):
        # ç§»åŠ¨æ•°æ®åˆ°GPU
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                for k, v in batch.items()}

        optimizer.zero_grad()

        try:
            # å‰å‘ä¼ æ’­
            outputs = model(batch)
            predicted_actions = outputs['actions']
            target_actions = batch['actions']

            # è®¡ç®—æŸå¤± (åªè€ƒè™‘å‰å‡ ç»´æœ‰æ•ˆåŠ¨ä½œ)
            # æ¯ä¸ªæ ·æœ¬å¯èƒ½æœ‰ä¸åŒçš„åŠ¨ä½œç»´åº¦
            batch_size = predicted_actions.size(0)
            chunk_size = predicted_actions.size(1)

            loss = 0.0
            valid_samples = 0

            for i in range(batch_size):
                # æ ¹æ®robot_configç¡®å®šæœ‰æ•ˆåŠ¨ä½œç»´åº¦
                robot_config = batch['robot_config'][i]
                if '3DOF' in robot_config or '3dof' in robot_config.lower():
                    action_dim = 6  # 3æ®µÃ—2å‚æ•°
                elif '4DOF' in robot_config or '4dof' in robot_config.lower():
                    action_dim = 8  # 4æ®µÃ—2å‚æ•°
                else:
                    action_dim = 10  # é»˜è®¤5æ®µÃ—2å‚æ•°

                # è®¡ç®—æœ‰æ•ˆåŠ¨ä½œçš„MSEæŸå¤±
                pred_valid = predicted_actions[i, :, :action_dim]
                target_valid = target_actions[i, :, :action_dim]

                sample_loss = criterion(pred_valid, target_valid)
                loss += sample_loss
                valid_samples += 1

            if valid_samples > 0:
                loss = loss / valid_samples
            else:
                loss = torch.tensor(0.0, device=device, requires_grad=True)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            if config.get('grad_clip_norm'):
                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip_norm'])

            optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            total_samples += batch_size

            if rank == 0 and step % config['log_interval'] == 0:
                pbar.set_postfix({
                    'loss': f'{loss.item():.6f}',
                    'avg_loss': f'{total_loss / (step + 1):.6f}'
                })

        except Exception as e:
            print(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
            continue

    avg_loss = total_loss / len(train_loader)
    return {'train_loss': avg_loss}

def validate(model: nn.Module,
             val_loader,
             criterion: nn.Module,
             device: torch.device,
             rank: int = 0) -> Dict[str, float]:
    """éªŒè¯æ¨¡å‹"""

    model.eval()
    total_loss = 0.0
    total_samples = 0

    with torch.no_grad():
        for batch in val_loader:
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()}

            try:
                outputs = model(batch)
                predicted_actions = outputs['actions']
                target_actions = batch['actions']

                # åŒè®­ç»ƒæ—¶çš„æŸå¤±è®¡ç®—
                batch_size = predicted_actions.size(0)
                loss = 0.0
                valid_samples = 0

                for i in range(batch_size):
                    robot_config = batch['robot_config'][i]
                    if '3DOF' in robot_config or '3dof' in robot_config.lower():
                        action_dim = 6
                    elif '4DOF' in robot_config or '4dof' in robot_config.lower():
                        action_dim = 8
                    else:
                        action_dim = 10

                    pred_valid = predicted_actions[i, :, :action_dim]
                    target_valid = target_actions[i, :, :action_dim]

                    sample_loss = criterion(pred_valid, target_valid)
                    loss += sample_loss
                    valid_samples += 1

                if valid_samples > 0:
                    loss = loss / valid_samples

                total_loss += loss.item()
                total_samples += batch_size

            except Exception as e:
                if rank == 0:
                    print(f"âš ï¸ éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                continue

    avg_loss = total_loss / len(val_loader)
    return {'val_loss': avg_loss}

def save_checkpoint(model: nn.Module,
                   optimizer: torch.optim.Optimizer,
                   epoch: int,
                   metrics: Dict[str, float],
                   config: Dict[str, Any],
                   filepath: str):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.module.state_dict() if isinstance(model, DDP) else model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config,
    }

    torch.save(checkpoint, filepath)
    print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")

def main():
    parser = argparse.ArgumentParser(description='è½¯ä½“è‡‚OpenPiè®­ç»ƒ')
    parser.add_argument('--config-override', type=str, help='é…ç½®è¦†ç›–JSONæ–‡ä»¶')
    args = parser.parse_args()

    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, local_rank = init_distributed()
    device = torch.device(f'cuda:{local_rank}')

    # è·å–é…ç½®
    config = get_soft_arm_config()
    if args.config_override:
        with open(args.config_override) as f:
            override_config = json.load(f)
        config.update(override_config)

    if rank == 0:
        print("ğŸ¯ è½¯ä½“è‡‚OpenPiè®­ç»ƒé…ç½®:")
        for key, value in config.items():
            print(f"   {key}: {value}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(config['output_dir'], config['experiment_name'])
    if rank == 0:
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜é…ç½®
        with open(os.path.join(output_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if rank == 0:
        print("ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")

    train_loader, val_loader = create_soft_arm_dataloaders(
        batch_size=config['batch_size'],
        num_workers=config['num_workers'],
        train_split=config['train_split'],
        image_size=config['image_size'],
        action_chunk_size=config['action_chunk_size'],
        max_sequence_length=config['max_sequence_length']
    )

    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    if world_size > 1:
        train_sampler = DistributedSampler(train_loader.dataset)
        val_sampler = DistributedSampler(val_loader.dataset)

        train_loader = torch.utils.data.DataLoader(
            train_loader.dataset,
            batch_size=config['batch_size'],
            sampler=train_sampler,
            num_workers=config['num_workers'],
            pin_memory=True
        )

        val_loader = torch.utils.data.DataLoader(
            val_loader.dataset,
            batch_size=config['batch_size'],
            sampler=val_sampler,
            num_workers=config['num_workers'],
            pin_memory=True
        )

    # åˆ›å»ºæ¨¡å‹
    if rank == 0:
        print("ğŸ¤– åŠ è½½æ¨¡å‹...")

    model = load_openpi_model(config, device)

    # åˆ†å¸ƒå¼æ¨¡å‹
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank], find_unused_parameters=True)

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],
        betas=(config['beta1'], config['beta2'])
    )

    criterion = nn.MSELoss()

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config['num_epochs'] * len(train_loader)
    )

    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')

    for epoch in range(config['num_epochs']):
        if world_size > 1:
            train_sampler.set_epoch(epoch)

        # è®­ç»ƒ
        train_metrics = train_epoch(
            model, train_loader, optimizer, criterion, device, epoch, config, rank
        )

        # éªŒè¯
        val_metrics = validate(model, val_loader, criterion, device, rank)

        # å­¦ä¹ ç‡æ›´æ–°
        scheduler.step()

        if rank == 0:
            print(f"ğŸ“Š Epoch {epoch}:")
            print(f"   è®­ç»ƒæŸå¤±: {train_metrics['train_loss']:.6f}")
            print(f"   éªŒè¯æŸå¤±: {val_metrics['val_loss']:.6f}")
            print(f"   å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['val_loss'] < best_val_loss:
                best_val_loss = val_metrics['val_loss']
                best_model_path = os.path.join(output_dir, 'best_model.pth')
                save_checkpoint(model, optimizer, epoch, {**train_metrics, **val_metrics},
                              config, best_model_path)

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 5 == 0:
                checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch}.pth')
                save_checkpoint(model, optimizer, epoch, {**train_metrics, **val_metrics},
                              config, checkpoint_path)

    if rank == 0:
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"   æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")

    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    if world_size > 1:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()