#!/usr/bin/env python3
"""
OpenPiËΩØ‰ΩìËáÇ8Âç°ËÆ≠ÁªÉËÑöÊú¨
Âü∫‰∫éœÄ‚ÇÄ.‚ÇÖ DROIDÂæÆË∞ÉÁâàÊú¨ÔºåÈÄÇÈÖçËΩØ‰ΩìËáÇÊï∞ÊçÆ

‰ΩøÁî®ÊñπÊ≥ï:
    ÂçïGPU: python train_soft_arm_openpi_8gpu.py
    8GPU:  torchrun --nproc_per_node=8 train_soft_arm_openpi_8gpu.py
"""

import os
import sys
import argparse
import json
from datetime import datetime
from typing import Dict, Any, Optional

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import numpy as np
from tqdm import tqdm

# Ê∑ªÂä†OpenPiË∑ØÂæÑ
sys.path.append('/home/cx/AET_FOR_RL/vla/ÂèÇËÄÉÊ®°Âûã/openpi')
sys.path.append('/home/cx/AET_FOR_RL/vla')

from openpi_soft_arm_dataloader import create_soft_arm_dataloaders
from openpi.training import config as openpi_config

def init_distributed():
    """ÂàùÂßãÂåñÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])

        print(f"üöÄ ÂàùÂßãÂåñÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ: rank={rank}, world_size={world_size}, local_rank={local_rank}")

        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(local_rank)

        return rank, world_size, local_rank
    else:
        print("üîß ÂçïGPUËÆ≠ÁªÉÊ®°Âºè")
        return 0, 1, 0

def get_soft_arm_config() -> Dict[str, Any]:
    """Ëé∑ÂèñËΩØ‰ΩìËáÇËÆ≠ÁªÉÈÖçÁΩÆ"""
    return {
        # Ê®°ÂûãÈÖçÁΩÆ
        'model_name': 'pi05_droid',  # ‰ΩøÁî®DROIDÂæÆË∞ÉÁâàÊú¨
        'pretrained_checkpoint': '~/.cache/openpi/checkpoints/pi05_droid',

        # Êï∞ÊçÆÈÖçÁΩÆ
        'batch_size': 8,  # ÊØèGPUÊâπÈáèÂ§ßÂ∞è
        'num_workers': 4,
        'train_split': 0.9,
        'image_size': (224, 224),
        'action_chunk_size': 16,
        'max_sequence_length': 50,

        # ËÆ≠ÁªÉÈÖçÁΩÆ
        'num_epochs': 20,
        'learning_rate': 1e-4,  # ÊØîÈ¢ÑËÆ≠ÁªÉÊõ¥Â∞èÁöÑÂ≠¶‰π†Áéá
        'weight_decay': 1e-5,
        'warmup_steps': 500,
        'save_interval': 1000,
        'eval_interval': 500,
        'log_interval': 50,

        # ËΩØ‰ΩìËáÇ‰∏ìÁî®
        'max_action_dim': 12,  # ÊîØÊåÅÊúÄÂ§ö6ÊÆµ√ó2ÂèÇÊï∞
        'constraint_types': ['3DOF', '4DOF'],

        # ‰ºòÂåñÂô®
        'optimizer': 'adamw',
        'beta1': 0.9,
        'beta2': 0.95,
        'grad_clip_norm': 1.0,

        # ËæìÂá∫ÁõÆÂΩï
        'output_dir': '/home/cx/AET_FOR_RL/vla/checkpoints/soft_arm_openpi',
        'experiment_name': f"soft_arm_pi05_droid_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    }

class SoftArmVLAModel(nn.Module):
    """ËΩØ‰ΩìËáÇVLAÊ®°ÂûãÈÄÇÈÖçÂô®"""

    def __init__(self, base_model, max_action_dim: int = 12):
        super().__init__()
        self.base_model = base_model
        self.max_action_dim = max_action_dim

        # Ëé∑ÂèñÂéüÂßãÂä®‰ΩúÂ§¥ÁöÑËæìÂÖ•Áª¥Â∫¶
        if hasattr(base_model, 'action_head'):
            original_dim = base_model.action_head.in_features
            self.action_head = nn.Linear(original_dim, max_action_dim)
        else:
            # Â¶ÇÊûúÊâæ‰∏çÂà∞Âä®‰ΩúÂ§¥ÔºåÂàõÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑÈÄÇÈÖçÂ±Ç
            self.action_head = nn.Linear(768, max_action_dim)  # ÂÅáËÆæ768Áª¥ÁâπÂæÅ

        print(f"‚úÖ ËΩØ‰ΩìËáÇÂä®‰ΩúÂ§¥: {self.action_head}")

    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ÂâçÂêë‰º†Êí≠"""
        # ‰ΩøÁî®Âü∫Á°ÄÊ®°ÂûãÊèêÂèñÁâπÂæÅ
        features = self.base_model.encode(
            images=batch['image'],
            instructions=batch['instruction']
        )

        # ÁîüÊàêËΩØ‰ΩìËáÇÂä®‰Ωú
        actions = self.action_head(features)

        return {
            'actions': actions,
            'features': features,
        }

def load_openpi_model(config: Dict[str, Any], device: torch.device):
    """Âä†ËΩΩOpenPiÊ®°Âûã"""
    try:
        # Ëé∑ÂèñDROIDÈÖçÁΩÆ
        openpi_cfg = openpi_config.get_config('pi05_droid')
        print(f"‚úÖ Âä†ËΩΩOpenPiÈÖçÁΩÆ: pi05_droid")

        # ÂàõÂª∫Âü∫Á°ÄÊ®°Âûã
        from openpi.models import create_model
        base_model = create_model(openpi_cfg)

        # Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáç
        checkpoint_path = os.path.expanduser(config['pretrained_checkpoint'])
        if os.path.exists(checkpoint_path):
            print(f"üîΩ Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáç: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            base_model.load_state_dict(checkpoint['model'], strict=False)
            print("‚úÖ È¢ÑËÆ≠ÁªÉÊùÉÈáçÂä†ËΩΩÊàêÂäü")
        else:
            print(f"‚ö†Ô∏è È¢ÑËÆ≠ÁªÉÊùÉÈáç‰∏çÂ≠òÂú®: {checkpoint_path}")
            print("   Â∞Ü‰ΩøÁî®ÈöèÊú∫ÂàùÂßãÂåñ")

        # ÂàõÂª∫ËΩØ‰ΩìËáÇÈÄÇÈÖçÊ®°Âûã
        model = SoftArmVLAModel(base_model, config['max_action_dim'])
        model = model.to(device)

        return model

    except Exception as e:
        print(f"‚ùå Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}")
        raise

def train_epoch(model: nn.Module,
                train_loader,
                optimizer: torch.optim.Optimizer,
                criterion: nn.Module,
                device: torch.device,
                epoch: int,
                config: Dict[str, Any],
                rank: int = 0) -> Dict[str, float]:
    """ËÆ≠ÁªÉ‰∏Ä‰∏™epoch"""

    model.train()
    total_loss = 0.0
    total_samples = 0

    if rank == 0:
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    else:
        pbar = train_loader

    for step, batch in enumerate(pbar):
        # ÁßªÂä®Êï∞ÊçÆÂà∞GPU
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                for k, v in batch.items()}

        optimizer.zero_grad()

        try:
            # ÂâçÂêë‰º†Êí≠
            outputs = model(batch)
            predicted_actions = outputs['actions']
            target_actions = batch['actions']

            # ËÆ°ÁÆóÊçüÂ§± (Âè™ËÄÉËôëÂâçÂá†Áª¥ÊúâÊïàÂä®‰Ωú)
            # ÊØè‰∏™Ê†∑Êú¨ÂèØËÉΩÊúâ‰∏çÂêåÁöÑÂä®‰ΩúÁª¥Â∫¶
            batch_size = predicted_actions.size(0)
            chunk_size = predicted_actions.size(1)

            loss = 0.0
            valid_samples = 0

            for i in range(batch_size):
                # Ê†πÊçÆrobot_configÁ°ÆÂÆöÊúâÊïàÂä®‰ΩúÁª¥Â∫¶
                robot_config = batch['robot_config'][i]
                if '3DOF' in robot_config or '3dof' in robot_config.lower():
                    action_dim = 6  # 3ÊÆµ√ó2ÂèÇÊï∞
                elif '4DOF' in robot_config or '4dof' in robot_config.lower():
                    action_dim = 8  # 4ÊÆµ√ó2ÂèÇÊï∞
                else:
                    action_dim = 10  # ÈªòËÆ§5ÊÆµ√ó2ÂèÇÊï∞

                # ËÆ°ÁÆóÊúâÊïàÂä®‰ΩúÁöÑMSEÊçüÂ§±
                pred_valid = predicted_actions[i, :, :action_dim]
                target_valid = target_actions[i, :, :action_dim]

                sample_loss = criterion(pred_valid, target_valid)
                loss += sample_loss
                valid_samples += 1

            if valid_samples > 0:
                loss = loss / valid_samples
            else:
                loss = torch.tensor(0.0, device=device, requires_grad=True)

            # ÂèçÂêë‰º†Êí≠
            loss.backward()

            # Ê¢ØÂ∫¶Ë£ÅÂâ™
            if config.get('grad_clip_norm'):
                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip_norm'])

            optimizer.step()

            # ÁªüËÆ°
            total_loss += loss.item()
            total_samples += batch_size

            if rank == 0 and step % config['log_interval'] == 0:
                pbar.set_postfix({
                    'loss': f'{loss.item():.6f}',
                    'avg_loss': f'{total_loss / (step + 1):.6f}'
                })

        except Exception as e:
            print(f"‚ùå ËÆ≠ÁªÉÊ≠•È™§Â§±Ë¥•: {e}")
            continue

    avg_loss = total_loss / len(train_loader)
    return {'train_loss': avg_loss}

def validate(model: nn.Module,
             val_loader,
             criterion: nn.Module,
             device: torch.device,
             rank: int = 0) -> Dict[str, float]:
    """È™åËØÅÊ®°Âûã"""

    model.eval()
    total_loss = 0.0
    total_samples = 0

    with torch.no_grad():
        for batch in val_loader:
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()}

            try:
                outputs = model(batch)
                predicted_actions = outputs['actions']
                target_actions = batch['actions']

                # ÂêåËÆ≠ÁªÉÊó∂ÁöÑÊçüÂ§±ËÆ°ÁÆó
                batch_size = predicted_actions.size(0)
                loss = 0.0
                valid_samples = 0

                for i in range(batch_size):
                    robot_config = batch['robot_config'][i]
                    if '3DOF' in robot_config or '3dof' in robot_config.lower():
                        action_dim = 6
                    elif '4DOF' in robot_config or '4dof' in robot_config.lower():
                        action_dim = 8
                    else:
                        action_dim = 10

                    pred_valid = predicted_actions[i, :, :action_dim]
                    target_valid = target_actions[i, :, :action_dim]

                    sample_loss = criterion(pred_valid, target_valid)
                    loss += sample_loss
                    valid_samples += 1

                if valid_samples > 0:
                    loss = loss / valid_samples

                total_loss += loss.item()
                total_samples += batch_size

            except Exception as e:
                if rank == 0:
                    print(f"‚ö†Ô∏è È™åËØÅÊ≠•È™§Â§±Ë¥•: {e}")
                continue

    avg_loss = total_loss / len(val_loader)
    return {'val_loss': avg_loss}

def save_checkpoint(model: nn.Module,
                   optimizer: torch.optim.Optimizer,
                   epoch: int,
                   metrics: Dict[str, float],
                   config: Dict[str, Any],
                   filepath: str):
    """‰øùÂ≠òÊ£ÄÊü•ÁÇπ"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.module.state_dict() if isinstance(model, DDP) else model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config,
    }

    torch.save(checkpoint, filepath)
    print(f"‚úÖ Ê£ÄÊü•ÁÇπÂ∑≤‰øùÂ≠ò: {filepath}")

def main():
    parser = argparse.ArgumentParser(description='ËΩØ‰ΩìËáÇOpenPiËÆ≠ÁªÉ')
    parser.add_argument('--config-override', type=str, help='ÈÖçÁΩÆË¶ÜÁõñJSONÊñá‰ª∂')
    args = parser.parse_args()

    # ÂàùÂßãÂåñÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ
    rank, world_size, local_rank = init_distributed()
    device = torch.device(f'cuda:{local_rank}')

    # Ëé∑ÂèñÈÖçÁΩÆ
    config = get_soft_arm_config()
    if args.config_override:
        with open(args.config_override) as f:
            override_config = json.load(f)
        config.update(override_config)

    if rank == 0:
        print("üéØ ËΩØ‰ΩìËáÇOpenPiËÆ≠ÁªÉÈÖçÁΩÆ:")
        for key, value in config.items():
            print(f"   {key}: {value}")

    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    output_dir = os.path.join(config['output_dir'], config['experiment_name'])
    if rank == 0:
        os.makedirs(output_dir, exist_ok=True)

        # ‰øùÂ≠òÈÖçÁΩÆ
        with open(os.path.join(output_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)

    # ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®
    if rank == 0:
        print("üìä ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®...")

    train_loader, val_loader = create_soft_arm_dataloaders(
        batch_size=config['batch_size'],
        num_workers=config['num_workers'],
        train_split=config['train_split'],
        image_size=config['image_size'],
        action_chunk_size=config['action_chunk_size'],
        max_sequence_length=config['max_sequence_length']
    )

    # ÂàÜÂ∏ÉÂºèÈááÊ†∑Âô®
    if world_size > 1:
        train_sampler = DistributedSampler(train_loader.dataset)
        val_sampler = DistributedSampler(val_loader.dataset)

        train_loader = torch.utils.data.DataLoader(
            train_loader.dataset,
            batch_size=config['batch_size'],
            sampler=train_sampler,
            num_workers=config['num_workers'],
            pin_memory=True
        )

        val_loader = torch.utils.data.DataLoader(
            val_loader.dataset,
            batch_size=config['batch_size'],
            sampler=val_sampler,
            num_workers=config['num_workers'],
            pin_memory=True
        )

    # ÂàõÂª∫Ê®°Âûã
    if rank == 0:
        print("ü§ñ Âä†ËΩΩÊ®°Âûã...")

    model = load_openpi_model(config, device)

    # ÂàÜÂ∏ÉÂºèÊ®°Âûã
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank], find_unused_parameters=True)

    # ‰ºòÂåñÂô®ÂíåÊçüÂ§±ÂáΩÊï∞
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],
        betas=(config['beta1'], config['beta2'])
    )

    criterion = nn.MSELoss()

    # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config['num_epochs'] * len(train_loader)
    )

    # ËÆ≠ÁªÉÂæ™ÁéØ
    best_val_loss = float('inf')

    for epoch in range(config['num_epochs']):
        if world_size > 1:
            train_sampler.set_epoch(epoch)

        # ËÆ≠ÁªÉ
        train_metrics = train_epoch(
            model, train_loader, optimizer, criterion, device, epoch, config, rank
        )

        # È™åËØÅ
        val_metrics = validate(model, val_loader, criterion, device, rank)

        # Â≠¶‰π†ÁéáÊõ¥Êñ∞
        scheduler.step()

        if rank == 0:
            print(f"üìä Epoch {epoch}:")
            print(f"   ËÆ≠ÁªÉÊçüÂ§±: {train_metrics['train_loss']:.6f}")
            print(f"   È™åËØÅÊçüÂ§±: {val_metrics['val_loss']:.6f}")
            print(f"   Â≠¶‰π†Áéá: {scheduler.get_last_lr()[0]:.2e}")

            # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã
            if val_metrics['val_loss'] < best_val_loss:
                best_val_loss = val_metrics['val_loss']
                best_model_path = os.path.join(output_dir, 'best_model.pth')
                save_checkpoint(model, optimizer, epoch, {**train_metrics, **val_metrics},
                              config, best_model_path)

            # ÂÆöÊúü‰øùÂ≠òÊ£ÄÊü•ÁÇπ
            if (epoch + 1) % 5 == 0:
                checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch}.pth')
                save_checkpoint(model, optimizer, epoch, {**train_metrics, **val_metrics},
                              config, checkpoint_path)

    if rank == 0:
        print("üéâ ËÆ≠ÁªÉÂÆåÊàê!")
        print(f"   ÊúÄ‰Ω≥È™åËØÅÊçüÂ§±: {best_val_loss:.6f}")
        print(f"   Ê®°Âûã‰øùÂ≠òÂú®: {output_dir}")

    # Ê∏ÖÁêÜÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ
    if world_size > 1:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()