#!/usr/bin/env python3
"""
ä¿®å¤è®¾å¤‡æ”¾ç½®é—®é¢˜çš„æ¨¡å‹åŠ è½½æµ‹è¯•
"""

import os
import sys
os.environ['CUDA_VISIBLE_DEVICES'] = '3'

# Add paths
sys.path.append('/home/cx/AET_FOR_RL/vla/train')
sys.path.append('/home/cx/AET_FOR_RL/vla')
sys.path.append('/home/cx/AET_FOR_RL/MA-VLA/src')

import torch
import numpy as np
from pathlib import Path

def fix_model_device_placement(model, device):
    """ç¡®ä¿æ¨¡å‹çš„æ‰€æœ‰ç»„ä»¶éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š"""
    print("ğŸ”§ Fixing device placement issues...")
    
    # é€’å½’åœ°å°†æ‰€æœ‰å­æ¨¡å—ç§»åˆ°è®¾å¤‡ä¸Š
    model = model.to(device)
    
    # æ£€æŸ¥å¹¶ä¿®å¤embeddingå±‚
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Embedding):
            print(f"   Moving embedding {name} to {device}")
            module = module.to(device)
    
    # ç¡®ä¿æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒºéƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    for name, param in model.named_parameters():
        if param.device != device:
            print(f"   Moving parameter {name} to {device}")
            param.data = param.data.to(device)
    
    for name, buffer in model.named_buffers():
        if buffer.device != device:
            print(f"   Moving buffer {name} to {device}")
            buffer.data = buffer.data.to(device)
            
    return model

def test_model_loading_with_fix():
    """æµ‹è¯•æ¨¡å‹åŠ è½½å¹¶ä¿®å¤è®¾å¤‡é—®é¢˜"""
    print("ğŸ§ª Testing VLA Model Loading (with device fix)")
    print("=" * 50)
    
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ Device: {device}")
    
    # å¯¼å…¥æ¨¡å‹
    try:
        from ma_vla_core import MA_VLA_Agent, RobotConfig
        print("âœ… Successfully imported MA_VLA_Agent")
    except Exception as e:
        print(f"âŒ Failed to import model: {e}")
        return None
        
    # åŠ è½½checkpoint
    model_path = "/home/cx/AET_FOR_RL/MA-VLA/checkpoints/ma_vla_final.pt"
    
    try:
        print("\nğŸ“¦ Loading checkpoint...")
        checkpoint = torch.load(model_path, map_location='cpu')
        print(f"âœ… Checkpoint loaded")
        print(f"   Keys: {list(checkpoint.keys())}")
        
        # è·å–é…ç½®
        max_dof = checkpoint.get('max_dof', 14)
        vision_language_dim = checkpoint.get('vision_language_dim', 512)
        
        print(f"   max_dof: {max_dof}")
        print(f"   vision_language_dim: {vision_language_dim}")
        
        # åˆ›å»ºæ¨¡å‹
        print("\nğŸ¤– Creating model instance...")
        model = MA_VLA_Agent(
            max_dof=max_dof,
            observation_dim=vision_language_dim,
            hidden_dim=256
        )
        
        # åŠ è½½æƒé‡
        if 'state_dict' in checkpoint:
            model.load_state_dict(checkpoint['state_dict'], strict=False)
            print("âœ… Loaded state_dict")
        
        # ä¿®å¤è®¾å¤‡æ”¾ç½®é—®é¢˜
        model = fix_model_device_placement(model, device)
        model.eval()
        
        print(f"âœ… Model ready on {device}")
        
        # æµ‹è¯•forward pass
        print("\nğŸ§ª Testing forward pass...")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ - ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        dummy_observations = torch.randn(vision_language_dim).to(device)
        
        # åˆ›å»ºæœºå™¨äººé…ç½®
        robot_config = RobotConfig(
            name="Franka_Panda",
            dof=7,
            joint_types=["revolute"] * 7,
            joint_limits=[(-2.8973, 2.8973), (-1.7628, 1.7628), (-2.8973, 2.8973), 
                         (-3.0718, -0.0698), (-2.8973, 2.8973), (-0.0175, 3.7525), 
                         (-2.8973, 2.8973)],
            link_lengths=[0.333, 0.316, 0.384, 0.088, 0.107, 0.103, 0.0]
        )
        
        # ä¿®æ”¹MA-VLAæ¨¡å‹çš„forwardæ–¹æ³•ä¸­çš„å¼ é‡åˆ›å»ºï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
        # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°
        def safe_forward(model, observations, robot_config):
            """å®‰å…¨çš„forward passï¼Œç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡"""
            # ä¿å­˜åŸå§‹çš„tensoråˆ›å»ºå‡½æ•°
            original_tensor = torch.tensor
            original_zeros = torch.zeros
            
            # åˆ›å»ºè®¾å¤‡æ„ŸçŸ¥çš„tensoråˆ›å»ºå‡½æ•°
            def device_aware_tensor(*args, **kwargs):
                if 'device' not in kwargs:
                    kwargs['device'] = device
                return original_tensor(*args, **kwargs)
            
            def device_aware_zeros(*args, **kwargs):
                if 'device' not in kwargs:
                    kwargs['device'] = device
                return original_zeros(*args, **kwargs)
            
            # ä¸´æ—¶æ›¿æ¢
            torch.tensor = device_aware_tensor
            torch.zeros = device_aware_zeros
            
            try:
                with torch.no_grad():
                    output = model(observations, robot_config)
                return output
            finally:
                # æ¢å¤åŸå§‹å‡½æ•°
                torch.tensor = original_tensor
                torch.zeros = original_zeros
        
        try:
            output = safe_forward(model, dummy_observations, robot_config)
            print(f"âœ… Forward pass successful!")
            print(f"   Actions shape: {output['actions'].shape}")
            print(f"   Values shape: {output['values'].shape}")
            print(f"   Actions device: {output['actions'].device}")
            
            # éªŒè¯è¾“å‡º
            actions = output['actions'].cpu().numpy()
            print(f"   Action range: [{actions.min():.3f}, {actions.max():.3f}]")
            
            return model, robot_config
            
        except Exception as e:
            print(f"âŒ Forward pass failed: {e}")
            print(f"   Error type: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return None, None
            
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        return None, None

def test_multi_morphology_inference(model, base_config):
    """æµ‹è¯•å¤šå½¢æ€æ¨ç†"""
    if model is None:
        return
        
    print("\nğŸ¤– Testing Multi-Morphology Inference")
    print("=" * 50)
    
    from ma_vla_core import RobotConfig  # Import here as well
    device = next(model.parameters()).device
    
    # æµ‹è¯•ä¸åŒçš„å½¢æ€é…ç½®
    morphology_configs = [
        ("5-DOF", 5, [0.3, 0.3, 0.3, 0.1, 0.1]),
        ("7-DOF", 7, [0.333, 0.316, 0.384, 0.088, 0.107, 0.103, 0.0]),
        ("8-DOF", 8, [0.3, 0.3, 0.3, 0.2, 0.2, 0.1, 0.1, 0.05])
    ]
    
    for name, dof, lengths in morphology_configs:
        print(f"\nğŸ“Š Testing {name}...")
        
        config = RobotConfig(
            name=name,
            dof=dof,
            joint_types=["revolute"] * dof,
            joint_limits=[(-3.14, 3.14)] * dof,
            link_lengths=lengths
        )
        
        # åˆ›å»ºè§‚å¯Ÿ
        observations = torch.randn(512).to(device)
        
        try:
            # ä½¿ç”¨monkey patchingç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
            original_tensor = torch.tensor
            torch.tensor = lambda *args, **kwargs: original_tensor(*args, **{**kwargs, 'device': device})
            
            with torch.no_grad():
                output = model(observations, config)
            
            torch.tensor = original_tensor
            
            print(f"   âœ… Success! Actions shape: {output['actions'].shape}")
            actions = output['actions'].cpu().numpy()
            print(f"   Action stats: mean={actions.mean():.3f}, std={actions.std():.3f}")
            
        except Exception as e:
            print(f"   âŒ Failed: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ MA-VLA Model Loading and Testing")
    print("This time with proper device handling!")
    print("=" * 50)
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    model, robot_config = test_model_loading_with_fix()
    
    if model is not None:
        print("\nâœ… Model loaded successfully!")
        
        # æµ‹è¯•å¤šå½¢æ€æ¨ç†
        test_multi_morphology_inference(model, robot_config)
        
        print("\nğŸ‰ SUCCESS!")
        print("Model is ready for real evaluation!")
        print("Next step: Run inference on actual DROID-100 test data")
    else:
        print("\nâŒ Model loading failed")
        print("Need to debug the device placement issues further")

if __name__ == "__main__":
    main()