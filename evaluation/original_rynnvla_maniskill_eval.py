#!/usr/bin/env python3
"""
Original RynnVLA-001 Model ManiSkill Evaluation
ä½¿ç”¨çº¯åŸå§‹RynnVLA-001æ¨¡å‹è¯„ä¼°ManiSkillï¼Œä¸ä½¿ç”¨æˆ‘ä»¬çš„GNNæ”¹è¿›
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os
from pathlib import Path

# ManiSkill imports
import mani_skill.envs
import gymnasium as gym
from mani_skill.utils import gym_utils

# Add RynnVLA path
sys.path.append('/home/cx/AET_FOR_RL/vla/å‚è€ƒæ¨¡å‹/RynnVLA-001')

class OriginalRynnVLAManiSkillEvaluator:
    """ä½¿ç”¨åŸå§‹RynnVLA-001æ¨¡å‹è¯„ä¼°ManiSkill"""

    def __init__(self, device: str = "cuda:0"):
        self.device = torch.device(device)

        print(f"ğŸ”¬ Original RynnVLA-001 ManiSkill Evaluator")
        print(f"   Device: {self.device}")

        # åˆå§‹åŒ–åŸå§‹RynnVLAæ¨¡å‹
        self.model = self._load_original_rynnvla()

        # åˆå§‹åŒ–ManiSkillç¯å¢ƒ
        self.env = self._setup_maniskill_env()

    def _load_original_rynnvla(self):
        """åŠ è½½åŸå§‹RynnVLA-001æ¨¡å‹"""
        print("ğŸ”„ Loading Original RynnVLA-001 model...")

        try:
            # å°è¯•å¯¼å…¥RynnVLAçš„åŸå§‹æ¨¡å‹ç±»
            from models.chameleon_model.chameleon.modeling_chameleon import ChameleonForConditionalGeneration
            from models.chameleon_model.configuration_xllmx_chameleon import ChameleonConfig

            # åˆ›å»ºé…ç½® - ä½¿ç”¨æ ‡å‡†çš„7ç»´actioné…ç½®
            config = ChameleonConfig(
                vocab_size=65536,
                hidden_size=4096,
                intermediate_size=11008,
                num_hidden_layers=32,
                num_attention_heads=32,
                num_key_value_heads=8,
                hidden_act="silu",
                max_position_embeddings=4096,
                initializer_range=0.02,
                rms_norm_eps=1e-5,
                use_cache=True,
                pad_token_id=0,
                bos_token_id=1,
                eos_token_id=2,
                action_chunk_size=20,
                action_dim=7,  # åŸå§‹çš„7ç»´action
                visual_head_type='one_layer',
                state_dim=6
            )

            # åˆ›å»ºæ¨¡å‹
            model = ChameleonForConditionalGeneration(config)
            model = model.to(self.device)
            model.eval()

            print("âœ… Original RynnVLA-001 model created")
            print(f"   Action dimension: {config.action_dim}")
            print(f"   Action chunk size: {config.action_chunk_size}")

            return model

        except Exception as e:
            print(f"âŒ Failed to load original RynnVLA: {e}")
            print("ğŸ”„ Creating simplified baseline model...")

            # å¦‚æœæ— æ³•åŠ è½½åŸå§‹æ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„baseline
            return self._create_baseline_model()

    def _create_baseline_model(self):
        """åˆ›å»ºç®€åŒ–çš„baselineæ¨¡å‹ç”¨äºæµ‹è¯•"""
        class SimpleVLABaseline(nn.Module):
            def __init__(self):
                super().__init__()
                self.action_dim = 7  # DROIDæ ‡å‡†ç»´åº¦
                self.action_chunk_size = 20

                # ç®€åŒ–çš„ç‰¹å¾æå–
                self.vision_encoder = nn.Sequential(
                    nn.Conv2d(3, 64, 7, 2, 3),
                    nn.ReLU(),
                    nn.AdaptiveAvgPool2d((8, 8)),
                    nn.Flatten(),
                    nn.Linear(64 * 64, 1024)
                )

                self.text_encoder = nn.Sequential(
                    nn.Linear(32, 512),
                    nn.ReLU(),
                    nn.Linear(512, 1024)
                )

                # Action head - è¾“å‡º7ç»´åŠ¨ä½œåºåˆ—
                self.action_head = nn.Sequential(
                    nn.Linear(2048, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, self.action_dim * self.action_chunk_size)
                )

            def forward(self, images, text_tokens):
                batch_size = images.shape[0]

                # ç‰¹å¾èåˆ
                vision_feat = self.vision_encoder(images)
                text_feat = self.text_encoder(text_tokens.float())
                fused_feat = torch.cat([vision_feat, text_feat], dim=-1)

                # ç”ŸæˆåŠ¨ä½œåºåˆ—
                action_logits = self.action_head(fused_feat)
                actions = action_logits.view(batch_size, self.action_chunk_size, self.action_dim)

                return {
                    'actions': actions,  # [batch, chunk_size, action_dim]
                    'logits': action_logits
                }

        model = SimpleVLABaseline().to(self.device)
        print("âœ… Baseline model created for testing")
        return model

    def _setup_maniskill_env(self):
        """è®¾ç½®ManiSkillç¯å¢ƒ"""
        print("ğŸ”„ Setting up ManiSkill environment...")

        try:
            env = gym.make(
                "PickCube-v1",
                obs_mode="state",
                control_mode="pd_joint_delta_pos",
                render_mode=None,
                max_episode_steps=200
            )

            print(f"âœ… ManiSkill environment created")
            print(f"   Task: PickCube-v1")
            print(f"   Action space: {env.action_space}")
            print(f"   Expected action dim: {env.action_space.shape[0]}")

            return env

        except Exception as e:
            print(f"âŒ Failed to setup ManiSkill environment: {e}")
            return None

    def _rynnvla_inference(self, obs, instruction="pick up the cube"):
        """ä½¿ç”¨åŸå§‹RynnVLAè¿›è¡Œæ¨ç†"""

        if self.model is None:
            # éšæœºåŠ¨ä½œä½œä¸ºfallback
            return np.random.uniform(-0.1, 0.1, size=8)

        try:
            with torch.no_grad():
                batch_size = 1

                # æ¨¡æ‹Ÿå›¾åƒè¾“å…¥ (å®é™…éƒ¨ç½²æ—¶éœ€è¦çœŸå®ç›¸æœºå›¾åƒ)
                images = torch.zeros(batch_size, 3, 320, 180, device=self.device)

                # æ¨¡æ‹Ÿæ–‡æœ¬token (å®é™…éœ€è¦proper tokenizer)
                text_tokens = torch.zeros(batch_size, 32, device=self.device)

                # è¿è¡ŒåŸå§‹RynnVLAæ¨ç†
                outputs = self.model(images, text_tokens)

                if 'actions' in outputs:
                    action_sequences = outputs['actions']  # [batch, chunk_size, action_dim]
                    print(f"   Model output shape: {action_sequences.shape}")

                    # æå–ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œ
                    first_step_action = action_sequences[0, 0, :].cpu().numpy()  # [action_dim]
                    print(f"   First step action shape: {first_step_action.shape}")
                    print(f"   Action values: {first_step_action}")

                    # è½¬æ¢ç»´åº¦ä»¥é€‚é…ManiSkill
                    return self._convert_to_maniskill_action(first_step_action)

                else:
                    print("âš ï¸ No 'actions' key in model output")
                    return np.random.uniform(-0.1, 0.1, size=8)

        except Exception as e:
            print(f"âš ï¸ RynnVLA inference failed: {e}")
            return np.random.uniform(-0.1, 0.1, size=8)

    def _convert_to_maniskill_action(self, rynnvla_action):
        """å°†RynnVLAè¾“å‡ºè½¬æ¢ä¸ºManiSkillæ ¼å¼"""
        rynnvla_dim = len(rynnvla_action)
        maniskill_dim = 8

        print(f"ğŸ”„ Converting action: {rynnvla_dim}D â†’ {maniskill_dim}D")
        print(f"   RynnVLA output: {rynnvla_action}")

        if rynnvla_dim == 7:
            # DROIDæ ¼å¼: [6å…³èŠ‚ + å¤¹çˆª] â†’ ManiSkill 8ç»´
            # å‡è®¾ManiSkillæ˜¯ [7å…³èŠ‚ + å¤¹çˆª]
            maniskill_action = np.zeros(8)
            maniskill_action[:6] = rynnvla_action[:6]  # å‰6ä¸ªå…³èŠ‚ç›´æ¥å¤åˆ¶
            maniskill_action[6] = 0.0  # ç¬¬7ä¸ªå…³èŠ‚è®¾ä¸º0 (æˆ–æ’å€¼)
            maniskill_action[7] = rynnvla_action[6]  # å¤¹çˆªæ˜ å°„åˆ°æœ€åä¸€ç»´

        elif rynnvla_dim == 6:
            # LeRobotæ ¼å¼: 6ç»´ â†’ 8ç»´
            maniskill_action = np.zeros(8)
            maniskill_action[:6] = rynnvla_action
            maniskill_action[6] = 0.0  # é¢å¤–å…³èŠ‚
            maniskill_action[7] = 0.5  # å¤¹çˆªè®¾ä¸ºä¸­æ€§

        elif rynnvla_dim == 8:
            # å·²ç»æ˜¯8ç»´ï¼Œç›´æ¥ä½¿ç”¨
            maniskill_action = rynnvla_action

        else:
            # å…¶ä»–ç»´åº¦ï¼Œæˆªæ–­æˆ–è¡¥é›¶
            maniskill_action = np.zeros(8)
            copy_dim = min(rynnvla_dim, 8)
            maniskill_action[:copy_dim] = rynnvla_action[:copy_dim]

        print(f"   ManiSkill action: {maniskill_action}")
        return maniskill_action

    def evaluate_single_episode(self, max_steps=200):
        """è¯„ä¼°å•ä¸ªepisode"""

        if self.env is None:
            return {"success": False, "error": "Environment not initialized"}

        obs, info = self.env.reset()
        episode_reward = 0
        steps = 0
        success = False

        print(f"ğŸ¯ Starting episode with Original RynnVLA-001")

        for step in range(max_steps):
            # ä½¿ç”¨åŸå§‹RynnVLAæ¨ç†
            action = self._rynnvla_inference(obs)

            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = self.env.step(action)

            episode_reward += reward
            steps += 1

            # æ£€æŸ¥æˆåŠŸæ¡ä»¶
            if info.get('success', False):
                success = True
                print(f"âœ… Success at step {steps}!")
                break

            if terminated or truncated:
                break

        result = {
            "success": success,
            "steps": steps,
            "reward": float(episode_reward),
            "model": "Original RynnVLA-001"
        }

        print(f"ğŸ“Š Episode result: Success={success}, Steps={steps}, Reward={float(episode_reward):.3f}")
        return result

    def run_evaluation(self, num_episodes=5):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸš€ Starting Original RynnVLA-001 ManiSkill Evaluation")
        print("=" * 60)

        if self.env is None:
            print("âŒ Cannot run evaluation - environment failed to initialize")
            return None

        results = []

        for episode in range(num_episodes):
            print(f"\nğŸ“… Episode {episode + 1}/{num_episodes}")
            result = self.evaluate_single_episode()
            results.append(result)

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        success_rate = np.mean([r["success"] for r in results])
        avg_reward = np.mean([r["reward"] for r in results])
        avg_steps = np.mean([r["steps"] for r in results])

        print(f"\nğŸ† Original RynnVLA-001 Results:")
        print(f"   Episodes: {num_episodes}")
        print(f"   Success Rate: {success_rate:.1%}")
        print(f"   Average Reward: {avg_reward:.3f}")
        print(f"   Average Steps: {avg_steps:.1f}")

        return {
            "results": results,
            "success_rate": success_rate,
            "avg_reward": avg_reward,
            "avg_steps": avg_steps
        }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ Original RynnVLA-001 ManiSkill Evaluation")
    print("=" * 50)

    evaluator = OriginalRynnVLAManiSkillEvaluator()
    evaluation_results = evaluator.run_evaluation(num_episodes=3)

    if evaluation_results:
        print(f"\nâœ… Evaluation completed!")
        print(f"ğŸ¯ This shows the original RynnVLA-001 performance on ManiSkill")
        print(f"ğŸ“Š Key insight: Action dimension conversion {7}D â†’ {8}D")
    else:
        print(f"\nâŒ Evaluation failed")

if __name__ == "__main__":
    main()