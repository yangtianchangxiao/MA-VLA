#!/usr/bin/env python3
"""
OpenPiè½¯ä½“è‡‚è®­ç»ƒé…ç½®
åŸºäºÏ€â‚€.â‚…æ¨¡å‹ï¼Œé€‚é…è½¯ä½“è‡‚æ•°æ®å’Œ8å¡è®­ç»ƒ
"""

import dataclasses
from typing import Dict, Any, Optional
import ml_collections

from openpi.training.config import TrainConfig, DataConfig, ModelConfig, OptimizerConfig

def get_soft_arm_config() -> ml_collections.ConfigDict:
    """è·å–è½¯ä½“è‡‚ä¸“ç”¨çš„OpenPiè®­ç»ƒé…ç½®"""

    # åŸºäºpi05_droidé…ç½®è¿›è¡Œä¿®æ”¹
    config = ml_collections.ConfigDict({
        # å®éªŒé…ç½®
        'exp_name': 'soft_arm_pi05',
        'description': 'Ï€â‚€.â‚… model fine-tuned on soft continuum arm data',

        # æ¨¡å‹é…ç½® - åŸºäºÏ€â‚€.â‚…
        'model_config': ml_collections.ConfigDict({
            'model_name': 'pi05',
            'pretrained_checkpoint': 'gs://openpi-assets/checkpoints/pi05_base',

            # è§†è§‰ç¼–ç å™¨
            'vision_encoder': ml_collections.ConfigDict({
                'image_size': 224,
                'patch_size': 16,
                'embed_dim': 768,
                'num_layers': 12,
                'num_heads': 12,
            }),

            # è¯­è¨€ç¼–ç å™¨
            'language_encoder': ml_collections.ConfigDict({
                'vocab_size': 32000,
                'embed_dim': 768,
                'num_layers': 12,
                'num_heads': 12,
                'max_seq_len': 128,
            }),

            # åŠ¨ä½œè§£ç å™¨ - é€‚é…è½¯ä½“è‡‚
            'action_decoder': ml_collections.ConfigDict({
                'action_dim': 10,  # æœ€å¤§5æ®µÃ—2å‚æ•° = 10ç»´
                'chunk_size': 16,  # åŠ¨ä½œé¢„æµ‹é•¿åº¦
                'use_flow_matching': True,  # Ï€â‚€.â‚…ç‰¹æ€§
                'num_flow_steps': 10,
            }),

            # è½¯ä½“è‡‚ä¸“ç”¨é…ç½®
            'soft_arm_config': ml_collections.ConfigDict({
                'max_segments': 5,
                'min_segments': 2,
                'joint_type': 'continuous',  # è¿ç»­ä½“å…³èŠ‚
                'constraint_types': ['3DOF', '4DOF'],  # æ”¯æŒçš„çº¦æŸç±»å‹
            }),
        }),

        # æ•°æ®é…ç½®
        'data_config': ml_collections.ConfigDict({
            'dataset_name': 'soft_arm_synthesis',
            'data_dir': '/home/cx/AET_FOR_RL/vla/synthesized_data',
            'image_dir': '/home/cx/AET_FOR_RL/vla/valid_original_data/droid_100/extracted_images',

            # æ•°æ®åŠ è½½
            'batch_size': 16,  # æ¯GPUæ‰¹é‡å¤§å°
            'num_workers': 8,
            'prefetch_factor': 2,
            'pin_memory': True,

            # æ•°æ®å¢å¼º
            'image_augmentation': ml_collections.ConfigDict({
                'random_crop': True,
                'color_jitter': True,
                'horizontal_flip': False,  # æœºå™¨äººæ•°æ®ä¸é€‚åˆç¿»è½¬
                'normalize': True,
            }),

            # åºåˆ—é…ç½®
            'sequence_length': 50,
            'action_chunk_size': 16,
            'language_conditioning': True,

            # è½¯ä½“è‡‚æ•°æ®ä¸“ç”¨
            'soft_arm_data': ml_collections.ConfigDict({
                'include_3dof': True,
                'include_4dof': True,
                'constraint_sampling': 'balanced',  # balanced, 3dof_heavy, 4dof_heavy
                'segment_sampling': 'uniform',      # uniform, weighted
            }),
        }),

        # ä¼˜åŒ–å™¨é…ç½®
        'optimizer_config': ml_collections.ConfigDict({
            'learning_rate': 3e-4,
            'warmup_steps': 1000,
            'weight_decay': 0.01,
            'beta1': 0.9,
            'beta2': 0.95,
            'grad_clip_norm': 1.0,

            # å­¦ä¹ ç‡è°ƒåº¦
            'lr_schedule': 'cosine',
            'lr_decay_steps': 50000,
            'min_lr_ratio': 0.1,
        }),

        # è®­ç»ƒé…ç½®
        'training_config': ml_collections.ConfigDict({
            'num_epochs': 50,
            'save_interval': 5000,
            'eval_interval': 1000,
            'log_interval': 100,
            'max_steps': 100000,

            # åˆ†å¸ƒå¼è®­ç»ƒ
            'use_fsdp': True,
            'fsdp_devices': 8,  # 8å¡è®­ç»ƒ
            'gradient_accumulation_steps': 1,

            # æ··åˆç²¾åº¦
            'use_amp': True,
            'amp_dtype': 'bfloat16',

            # æ­£åˆ™åŒ–
            'dropout_rate': 0.1,
            'layer_norm_eps': 1e-6,
        }),

        # è¯„ä¼°é…ç½®
        'eval_config': ml_collections.ConfigDict({
            'eval_batch_size': 8,
            'eval_steps': 100,
            'metrics': ['action_mse', 'action_l1', 'segment_accuracy'],

            # è½¯ä½“è‡‚ä¸“ç”¨è¯„ä¼°
            'soft_arm_metrics': ml_collections.ConfigDict({
                'position_error_threshold': 0.05,  # 5cm
                'orientation_error_threshold': 0.5,  # cos similarity
                'evaluate_by_segments': True,
                'evaluate_by_constraint': True,
            }),
        }),

        # æ£€æŸ¥ç‚¹é…ç½®
        'checkpoint_config': ml_collections.ConfigDict({
            'save_dir': '/home/cx/AET_FOR_RL/vla/checkpoints/soft_arm_pi05',
            'save_best': True,
            'save_latest': True,
            'keep_top_k': 3,
            'monitor_metric': 'eval/action_l1',
            'monitor_mode': 'min',
        }),

        # æ—¥å¿—é…ç½®
        'logging_config': ml_collections.ConfigDict({
            'use_wandb': True,
            'wandb_project': 'soft-arm-pi05',
            'wandb_entity': 'your-wandb-entity',
            'log_gradients': False,
            'log_activations': False,

            # å¯è§†åŒ–
            'log_images': True,
            'log_videos': False,
            'log_trajectory_plots': True,
        }),

        # ç¡¬ä»¶é…ç½®
        'hardware_config': ml_collections.ConfigDict({
            'device': 'cuda',
            'num_gpus': 8,
            'gpu_memory_limit': None,  # ä½¿ç”¨å…¨éƒ¨GPUå†…å­˜
            'num_cpu_threads': 32,
        }),

        # è°ƒè¯•é…ç½®
        'debug_config': ml_collections.ConfigDict({
            'debug_mode': False,
            'profile_training': False,
            'detect_anomaly': False,
            'log_device_placement': False,
        }),
    })

    return config

def get_soft_arm_data_config() -> Dict[str, Any]:
    """è·å–è½¯ä½“è‡‚æ•°æ®é…ç½®"""
    return {
        'data_sources': {
            'soft_arm_4dof': {
                'path': '/home/cx/AET_FOR_RL/vla/synthesized_data/soft_arm_4dof_synthesis',
                'episodes': 20,
                'configs_per_episode': 4,
                'constraint_type': '4DOF_relaxed',
                'weight': 0.3,  # 4DOFæ•°æ®æƒé‡è¾ƒä½ï¼ˆæ•°æ®å°‘ï¼‰
            },
            'soft_arm_3dof': {
                'path': '/home/cx/AET_FOR_RL/vla/synthesized_data/soft_arm_morphology_synthesis',
                'episodes': 46,
                'configs_per_episode': 4,
                'constraint_type': '3DOF',
                'weight': 0.7,  # 3DOFæ•°æ®æƒé‡è¾ƒé«˜ï¼ˆæ•°æ®å¤šï¼‰
            },
        },
        'image_sources': {
            'droid_extracted': {
                'path': '/home/cx/AET_FOR_RL/vla/valid_original_data/droid_100/extracted_images',
                'camera_views': ['exterior_image_1_left', 'exterior_image_2_left', 'wrist_image_left'],
                'preferred_view': 'exterior_image_1_left',
                'fallback_enabled': True,
            },
        },
        'task_descriptions': {
            'path': '/home/cx/AET_FOR_RL/vla/valid_original_data/droid_100/task_descriptions.json',
            'fallback_description': 'Complete the manipulation task with the soft continuum arm',
            'max_length': 128,
        },
    }

def validate_config(config: ml_collections.ConfigDict) -> bool:
    """éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§"""
    required_keys = [
        'model_config', 'data_config', 'optimizer_config',
        'training_config', 'checkpoint_config'
    ]

    for key in required_keys:
        if key not in config:
            print(f"âŒ ç¼ºå°‘å¿…è¦é…ç½®: {key}")
            return False

    # æ£€æŸ¥GPUæ•°é‡
    if config.training_config.fsdp_devices > 8:
        print("âš ï¸ GPUæ•°é‡è¶…è¿‡8å¼ ï¼Œè¯·è°ƒæ•´fsdp_devices")
        return False

    # æ£€æŸ¥æ‰¹é‡å¤§å°
    total_batch_size = config.data_config.batch_size * config.training_config.fsdp_devices
    if total_batch_size > 128:
        print(f"âš ï¸ æ€»æ‰¹é‡å¤§å° {total_batch_size} å¯èƒ½è¿‡å¤§ï¼Œå»ºè®®è°ƒæ•´")

    # æ£€æŸ¥æ•°æ®è·¯å¾„
    import os
    data_paths = [
        config.data_config.data_dir,
        config.data_config.image_dir,
    ]

    for path in data_paths:
        if not os.path.exists(path):
            print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {path}")
            return False

    print("âœ… é…ç½®éªŒè¯é€šè¿‡")
    return True

def create_training_script_template() -> str:
    """åˆ›å»ºè®­ç»ƒè„šæœ¬æ¨¡æ¿"""
    return '''#!/usr/bin/env python3
"""
OpenPiè½¯ä½“è‡‚è®­ç»ƒè„šæœ¬
åŸºäºÏ€â‚€.â‚…æ¨¡å‹çš„8å¡åˆ†å¸ƒå¼è®­ç»ƒ
"""

import os
import sys
sys.path.append('/home/cx/AET_FOR_RL/vla')

from openpi_soft_arm_config import get_soft_arm_config, validate_config
from openpi.training import config as openpi_config

def main():
    # è·å–é…ç½®
    config = get_soft_arm_config()

    # éªŒè¯é…ç½®
    if not validate_config(config):
        print("âŒ é…ç½®éªŒè¯å¤±è´¥")
        sys.exit(1)

    print("ğŸš€ å¼€å§‹OpenPiè½¯ä½“è‡‚è®­ç»ƒ")
    print(f"   æ¨¡å‹: {config.model_config.model_name}")
    print(f"   GPUæ•°é‡: {config.training_config.fsdp_devices}")
    print(f"   æ€»æ‰¹é‡å¤§å°: {config.data_config.batch_size * config.training_config.fsdp_devices}")

    # æ³¨å†Œé…ç½®åˆ°OpenPiç³»ç»Ÿ
    openpi_config.CONFIG_MAP['soft_arm_pi05'] = config

    # å¯¼å…¥å¹¶è¿è¡Œè®­ç»ƒ
    from openpi.scripts.train_pytorch import main as train_main
    train_main()

if __name__ == "__main__":
    main()
'''

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    config = get_soft_arm_config()
    data_config = get_soft_arm_data_config()

    print("ğŸ§ª æµ‹è¯•è½¯ä½“è‡‚OpenPié…ç½®")
    print(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
    print(f"   æ¨¡å‹: {config.model_config.model_name}")
    print(f"   GPUæ•°é‡: {config.training_config.fsdp_devices}")
    print(f"   æ‰¹é‡å¤§å°: {config.data_config.batch_size}")
    print(f"   æ£€æŸ¥ç‚¹ç›®å½•: {config.checkpoint_config.save_dir}")

    print(f"âœ… æ•°æ®é…ç½®:")
    for name, source in data_config['data_sources'].items():
        print(f"   {name}: {source['episodes']} episodes, weight={source['weight']}")

    # éªŒè¯é…ç½®
    validate_config(config)

    # åˆ›å»ºè®­ç»ƒè„šæœ¬
    script_content = create_training_script_template()
    with open('/home/cx/AET_FOR_RL/vla/train_soft_arm_pi05.py', 'w') as f:
        f.write(script_content)

    print("ğŸ“„ è®­ç»ƒè„šæœ¬å·²åˆ›å»º: train_soft_arm_pi05.py")