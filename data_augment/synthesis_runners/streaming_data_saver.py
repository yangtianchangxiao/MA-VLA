#!/usr/bin/env python3
"""
æµå¼æ•°æ®ä¿å­˜å™¨ - æ”¯æŒrobot graphå’Œtimestepæ•°æ®çš„æµå¼ä¿å­˜
é¿å…å†…å­˜ç§¯ç´¯ï¼Œè¾¹ç”Ÿæˆè¾¹ä¿å­˜
"""

import os
import json
import random
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from pathlib import Path


def generate_random_joint_limits(num_joints: int = 7) -> List[Tuple[float, float]]:
    """ç”ŸæˆéšæœºåŒ–çš„å…³èŠ‚é™åˆ¶ - æ­£è´Ÿåˆ†åˆ«éšæœºåŒ–"""
    joint_limits = []
    for i in range(num_joints):
        lower_limit = -random.uniform(2.9, 3.07)  # è´Ÿæ–¹å‘é™åˆ¶
        upper_limit = random.uniform(2.9, 3.07)   # æ­£æ–¹å‘é™åˆ¶
        joint_limits.append((lower_limit, upper_limit))
    return joint_limits


def generate_chain_adjacency(num_nodes: int) -> List[List[int]]:
    """ç”Ÿæˆé“¾å¼æœºå™¨äººçš„é‚»æ¥çŸ©é˜µ"""
    adj = [[0] * num_nodes for _ in range(num_nodes)]
    for i in range(num_nodes - 1):
        adj[i][i + 1] = 1  # forward connection
        adj[i + 1][i] = 1  # backward connection
    return adj


def generate_robot_graph(morphology_data: Dict, joint_limits: List[Tuple[float, float]]) -> Dict:
    """ç”Ÿæˆrobot graphç»“æ„"""
    # Node features: [lower_limit, upper_limit, velocity_limit, link_length, node_type]
    node_features = []

    # Base node (type=0)
    node_features.append([0.0, 0.0, 0.0, 0.0, 0])

    # Joint nodes
    for i, (lower, upper) in enumerate(joint_limits):
        # è·å–linké•¿åº¦
        if 'dh_parameters' in morphology_data and len(morphology_data['dh_parameters']) > i:
            link_length = morphology_data['dh_parameters'][i][2]  # DHå‚æ•°ä¸­çš„aå‚æ•°
        else:
            link_length = 0.33  # é»˜è®¤é•¿åº¦

        # æœ€åä¸€ä¸ªå…³èŠ‚æ˜¯end effector (type=2)ï¼Œå…¶ä»–æ˜¯intermediate (type=1)
        node_type = 2 if i == len(joint_limits) - 1 else 1

        node_features.append([lower, upper, 2.175, link_length, node_type])

    # ç”Ÿæˆé‚»æ¥çŸ©é˜µ (base + joints)
    num_nodes = len(node_features)
    adjacency_matrix = generate_chain_adjacency(num_nodes)

    return {
        "num_nodes": num_nodes,
        "node_features": node_features,
        "adjacency_matrix": adjacency_matrix,
        "dh_parameters": morphology_data.get('dh_parameters', []),
        "morphology_type": morphology_data.get('type', 'unknown')
    }


class StreamingSynthesisDataSaver:
    """æµå¼æ•°æ®ä¿å­˜å™¨ - é¿å…å†…å­˜ç§¯ç´¯"""

    def __init__(self, output_dir: str, morphology_type: str):
        self.output_dir = Path(output_dir)
        self.morphology_type = morphology_type

        # åˆ›å»ºç›®å½•ç»“æ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.graphs_dir = self.output_dir / "robot_graphs"
        self.graphs_dir.mkdir(exist_ok=True)

        # è½¨è¿¹æ•°æ®æ–‡ä»¶
        self.trajectory_file = self.output_dir / "trajectory_data.parquet"
        self._trajectory_buffer = []
        self._buffer_size = 100  # æ‰¹é‡å†™å…¥å¤§å°

        print(f"ğŸ’¾ StreamingSynthesisDataSaver initialized")
        print(f"   Output directory: {self.output_dir}")
        print(f"   Graphs directory: {self.graphs_dir}")

    def save_robot_graph(self, graph: Dict, graph_id: str):
        """ç«‹å³ä¿å­˜å•ä¸ªrobot graph"""
        graph_file = self.graphs_dir / f"{graph_id}_graph.json"

        with open(graph_file, 'w') as f:
            json.dump(graph, f, indent=2, default=self._json_serializer)

        print(f"   ğŸ“Š Saved robot graph: {graph_id}")

    def append_timestep_data(self, timestep_data: Dict):
        """æµå¼è¿½åŠ timestepæ•°æ®"""
        self._trajectory_buffer.append(timestep_data)

        # è¾¾åˆ°bufferå¤§å°æ—¶å†™å…¥ç£ç›˜
        if len(self._trajectory_buffer) >= self._buffer_size:
            self._flush_trajectory_buffer()

    def _flush_trajectory_buffer(self):
        """å°†bufferä¸­çš„æ•°æ®å†™å…¥parquetæ–‡ä»¶"""
        if not self._trajectory_buffer:
            return

        df = pd.DataFrame(self._trajectory_buffer)

        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ ï¼›å¦åˆ™åˆ›å»ºæ–°æ–‡ä»¶
        if self.trajectory_file.exists():
            existing_df = pd.read_parquet(self.trajectory_file)
            df = pd.concat([existing_df, df], ignore_index=True)

        df.to_parquet(self.trajectory_file, index=False)

        print(f"   ğŸ’¾ Flushed {len(self._trajectory_buffer)} timesteps to {self.trajectory_file.name}")
        self._trajectory_buffer.clear()

    def finalize(self):
        """å®Œæˆä¿å­˜ï¼Œæ¸…ç†å‰©ä½™buffer"""
        if self._trajectory_buffer:
            self._flush_trajectory_buffer()

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'morphology_type': self.morphology_type,
            'total_graphs': len(list(self.graphs_dir.glob("*_graph.json"))),
            'trajectory_file': str(self.trajectory_file),
            'buffer_size_used': self._buffer_size
        }

        metadata_file = self.output_dir / "synthesis_metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"   âœ… Synthesis data finalized: {metadata['total_graphs']} graphs saved")

    def _json_serializer(self, obj):
        """JSONåºåˆ—åŒ–è¾…åŠ©å‡½æ•°"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        return obj