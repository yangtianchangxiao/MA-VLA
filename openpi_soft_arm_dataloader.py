#!/usr/bin/env python3
"""
OpenPiè½¯ä½“è‡‚æ•°æ®åŠ è½½å™¨
å°†è½¯ä½“è‡‚NPZæ•°æ®è½¬æ¢ä¸ºOpenPiè®­ç»ƒæ ¼å¼
"""

import os
import json
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Any, Optional
import cv2
from pathlib import Path

class SoftArmOpenPiDataset(Dataset):
    """è½¯ä½“è‡‚æ•°æ®é›†ï¼Œå…¼å®¹OpenPiè®­ç»ƒæ ¼å¼"""

    def __init__(self,
                 soft_arm_data_dir: str = "/home/cx/AET_FOR_RL/vla/synthesized_data",
                 droid_images_dir: str = "/home/cx/AET_FOR_RL/vla/valid_original_data/droid_100/extracted_images",
                 image_size: Tuple[int, int] = (224, 224),
                 max_sequence_length: int = 50,
                 action_chunk_size: int = 16):

        self.image_size = image_size
        self.max_sequence_length = max_sequence_length
        self.action_chunk_size = action_chunk_size

        # åŠ è½½æ‰€æœ‰æ•°æ®é›†
        self.episodes = []

        # 3DOFæ•°æ® (æ›´å¤šæ•°æ®)
        dof3_dir = os.path.join(soft_arm_data_dir, "soft_arm_morphology_synthesis")
        if os.path.exists(dof3_dir):
            self._load_episodes(dof3_dir, "3DOF", droid_images_dir)

        # 4DOFæ•°æ®
        dof4_dir = os.path.join(soft_arm_data_dir, "soft_arm_4dof_synthesis")
        if os.path.exists(dof4_dir):
            self._load_episodes(dof4_dir, "4DOF", droid_images_dir)

        print(f"âœ… åŠ è½½äº† {len(self.episodes)} ä¸ªè®­ç»ƒåºåˆ—")

        # ç»Ÿè®¡ä¿¡æ¯
        if len(self.episodes) > 0:
            total_timesteps = sum(len(ep['action_chunk']) for ep in self.episodes)
            print(f"   æ€»æ—¶é—´æ­¥: {total_timesteps}")
            print(f"   å¹³å‡åºåˆ—é•¿åº¦: {total_timesteps / len(self.episodes):.1f}")
        else:
            print("   æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")

    def _load_episodes(self, data_dir: str, constraint_type: str, image_dir: str):
        """åŠ è½½æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰episodeæ•°æ®"""

        # å¤„ç†åˆ†å±‚ç›®å½•ç»“æ„: episode_xxx/n_segments/joint_trajectory.npz
        for episode_dir in os.listdir(data_dir):
            if not episode_dir.startswith('episode_'):
                continue

            episode_path = os.path.join(data_dir, episode_dir)
            if not os.path.isdir(episode_path):
                continue

            episode_id = episode_dir  # episode_000, episode_001, etc.

            # éå†æ¯ä¸ªæ®µæ•°é…ç½®
            for segment_dir in os.listdir(episode_path):
                if not segment_dir.endswith('_segments'):
                    continue

                segment_path = os.path.join(episode_path, segment_dir)
                if not os.path.isdir(segment_path):
                    continue

                # åŠ è½½å…³èŠ‚è½¨è¿¹æ–‡ä»¶
                trajectory_file = os.path.join(segment_path, 'joint_trajectory.npz')
                config_file = os.path.join(segment_path, 'config.json')

                if not os.path.exists(trajectory_file):
                    continue

                try:
                    # åŠ è½½è½¨è¿¹æ•°æ®
                    data = np.load(trajectory_file)

                    # åŠ è½½é…ç½®ä¿¡æ¯
                    robot_config = f"{segment_dir}_{constraint_type}"
                    task_description = f"Complete manipulation task using {constraint_type} soft continuum arm with {segment_dir}"

                    if os.path.exists(config_file):
                        with open(config_file, 'r') as f:
                            config_data = json.load(f)
                            if 'task_description' in config_data:
                                task_description = str(config_data['task_description'])
                            robot_config = f"{segment_dir}_{constraint_type}_{config_data.get('robot_id', 'default')}"

                    # æå–æ•°æ®
                    actions = data['joint_positions']  # è½¯ä½“è‡‚å…³èŠ‚è§’åº¦ (N, action_dim)
                    ee_positions = data['end_effector_positions']  # (N, 3)
                    ee_orientations = data.get('end_effector_orientations', None)  # (N, 3)

                    # ä¸ºæ¯ä¸ªæ—¶é—´æ­¥åˆ›å»ºè®­ç»ƒæ ·æœ¬
                    for i in range(len(actions)):
                        # æ‰¾åˆ°å¯¹åº”çš„DROIDå›¾åƒ
                        image_path = self._find_matching_image(
                            episode_id, i, image_dir, ee_positions[i]
                        )

                        if image_path is None:
                            continue

                        # åˆ›å»ºåŠ¨ä½œchunk (å½“å‰+æœªæ¥åŠ¨ä½œ)
                        action_chunk = self._create_action_chunk(actions, i)

                        episode_data = {
                            'episode_id': episode_id,
                            'robot_config': robot_config,
                            'constraint_type': constraint_type,
                            'timestep': i,
                            'image_path': image_path,
                            'task_description': task_description,
                            'action_chunk': action_chunk,
                            'ee_position': ee_positions[i],
                            'ee_orientation': ee_orientations[i] if ee_orientations is not None else None,
                        }

                        self.episodes.append(episode_data)

                except Exception as e:
                    print(f"âš ï¸ åŠ è½½episodeå¤±è´¥: {trajectory_file}, é”™è¯¯: {e}")
                    continue

    def _find_matching_image(self, episode_id: str, timestep: int,
                           image_dir: str, ee_pos: np.ndarray) -> Optional[str]:
        """æ‰¾åˆ°åŒ¹é…çš„DROIDå›¾åƒ"""

        # ä»episode_idæå–åŸå§‹DROID episodeå·
        if 'episode_' in episode_id:
            original_episode = episode_id.split('episode_')[1].split('_')[0]
        else:
            original_episode = episode_id.split('_')[0]

        # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
        episode_dir = os.path.join(image_dir, f"episode_{original_episode}")
        if not os.path.exists(episode_dir):
            return None

        # ä¼˜å…ˆé€‰æ‹©å¤–éƒ¨ç›¸æœºè§†è§’
        camera_views = ['exterior_image_1_left', 'exterior_image_2_left', 'wrist_image_left']

        for camera in camera_views:
            camera_dir = os.path.join(episode_dir, camera)
            if os.path.exists(camera_dir):
                # ä½¿ç”¨æ—¶é—´æ­¥åŒ¹é…æˆ–å°±è¿‘é€‰æ‹©
                images = sorted([f for f in os.listdir(camera_dir) if f.endswith('.jpg')])
                if images:
                    # ç®€å•çš„æ—¶é—´æ­¥åŒ¹é…
                    img_idx = min(timestep, len(images) - 1)
                    return os.path.join(camera_dir, images[img_idx])

        return None

    def _create_action_chunk(self, actions: np.ndarray, start_idx: int) -> np.ndarray:
        """åˆ›å»ºåŠ¨ä½œchunkç”¨äºè®­ç»ƒ"""

        end_idx = min(start_idx + self.action_chunk_size, len(actions))
        action_chunk = actions[start_idx:end_idx]

        # å¦‚æœchunkä¸å¤Ÿé•¿ï¼Œç”¨æœ€åä¸€ä¸ªåŠ¨ä½œå¡«å……
        if len(action_chunk) < self.action_chunk_size:
            last_action = action_chunk[-1]
            padding = np.tile(last_action, (self.action_chunk_size - len(action_chunk), 1))
            action_chunk = np.concatenate([action_chunk, padding], axis=0)

        return action_chunk

    def __len__(self) -> int:
        return len(self.episodes)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        episode = self.episodes[idx]

        # åŠ è½½å›¾åƒ
        image = self._load_and_preprocess_image(episode['image_path'])

        # åŠ¨ä½œæ•°æ®
        actions = torch.from_numpy(episode['action_chunk']).float()

        return {
            'image': image,
            'instruction': episode['task_description'],
            'actions': actions,
            'robot_config': episode['robot_config'],
            'constraint_type': episode['constraint_type'],
            'episode_id': episode['episode_id'],
            'timestep': episode['timestep'],
        }

    def _load_and_preprocess_image(self, image_path: str) -> torch.Tensor:
        """åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                # åˆ›å»ºå ä½å›¾åƒ
                image = np.zeros((224, 224, 3), dtype=np.uint8)
            else:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image = cv2.resize(image, self.image_size)

            # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
            image = torch.from_numpy(image).float() / 255.0
            image = image.permute(2, 0, 1)  # HWC -> CHW

            return image

        except Exception as e:
            print(f"âš ï¸ å›¾åƒåŠ è½½å¤±è´¥: {image_path}, é”™è¯¯: {e}")
            # è¿”å›å ä½å›¾åƒ
            return torch.zeros(3, self.image_size[0], self.image_size[1])

def create_soft_arm_dataloaders(
    batch_size: int = 8,
    num_workers: int = 4,
    train_split: float = 0.8,
    **dataset_kwargs
) -> Tuple[DataLoader, DataLoader]:
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""

    dataset = SoftArmOpenPiDataset(**dataset_kwargs)

    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    train_size = int(len(dataset) * train_split)
    val_size = len(dataset) - train_size

    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )

    print(f"âœ… æ•°æ®åˆ†å‰²: è®­ç»ƒ {len(train_dataset)}, éªŒè¯ {len(val_dataset)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )

    return train_loader, val_loader

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print("ğŸ§ª æµ‹è¯•è½¯ä½“è‡‚æ•°æ®åŠ è½½å™¨...")

    dataset = SoftArmOpenPiDataset()

    if len(dataset) > 0:
        sample = dataset[0]
        print(f"âœ… æ ·æœ¬æ•°æ®å½¢çŠ¶:")
        print(f"   å›¾åƒ: {sample['image'].shape}")
        print(f"   åŠ¨ä½œ: {sample['actions'].shape}")
        print(f"   æŒ‡ä»¤: {sample['instruction'][:100]}...")
        print(f"   æœºå™¨äººé…ç½®: {sample['robot_config']}")

        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_soft_arm_dataloaders(batch_size=4)
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
        print(f"   éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")

        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(train_loader))
        print(f"âœ… æ‰¹æ¬¡æ•°æ®:")
        print(f"   å›¾åƒæ‰¹æ¬¡: {batch['image'].shape}")
        print(f"   åŠ¨ä½œæ‰¹æ¬¡: {batch['actions'].shape}")
        print(f"   æŒ‡ä»¤æ•°é‡: {len(batch['instruction'])}")

    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")